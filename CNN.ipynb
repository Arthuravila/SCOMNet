{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c79b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import syft as sy\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import ujson as json\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d34f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(path, screenName):\n",
    "\n",
    "    users = [ f.path for f in os.scandir(path) if f.is_dir() ]\n",
    "    info = pd.DataFrame(columns= ['accelometer_size', 'gyroscope_size', 'timestamp'])\n",
    "\n",
    "    accelerometer = pd.DataFrame(columns=['x', 'y', 'z', 'screen', 'user', 'magnitude','combine_angle', 'timestamp'])\n",
    "    gyroscope = pd.DataFrame(columns=['x_gyroscope', 'y_gyroscope', 'z_gyroscope', 'screen_gyroscope', 'user_gyroscope', 'magnitude_gyroscope', 'combine_angle_gyroscope', 'timestamp_gyroscope'])\n",
    "\n",
    "    # Read sensors data from json file and save them in Dataframes\n",
    "    for i in range(0, len(users)):\n",
    "\n",
    "        json_files = [pos_json for pos_json in os.listdir(users[i]) if pos_json.endswith('.json')]\n",
    "\n",
    "        for index, js in enumerate(json_files):\n",
    "            with open(os.path.join(users[i], js)) as json_file:\n",
    "                json_text = json.load(json_file)\n",
    "                accSize = 0\n",
    "                gyrSize = 0\n",
    "                js = js.replace('.json','')\n",
    "                arr = js.split('_')\n",
    "\n",
    "                for j in json_text['accelerometer']:\n",
    "                    if screenName in j['screen']:\n",
    "                        x = j['x']\n",
    "                        y = j['y']\n",
    "                        z = j['z']\n",
    "                        if x == 0 and y == 0:\n",
    "                            continue\n",
    "                        screen = j['screen']\n",
    "                        user = arr[0]\n",
    "                        m = x**2 + y**2 + z**2\n",
    "                        m = np.sqrt(m)\n",
    "                        ca = np.sqrt(y**2 + z**2)\n",
    "                        timestamp = arr[1]\n",
    "                        accSize = accSize + 1\n",
    "                        df = {'x': x, 'y': y, 'z' : z, 'screen' : screen, 'user': user, 'magnitude' : m, 'combine_angle': ca, 'timestamp': timestamp}\n",
    "                        accelerometer = accelerometer.append(df, ignore_index=True)\n",
    "                        \n",
    "                for j in json_text['gyroscope']:\n",
    "                    if screenName in j['screen']:\n",
    "                        x = j['x']\n",
    "                        y = j['y']\n",
    "                        z = j['z']\n",
    "                        if x == 0 and y == 0:\n",
    "                            continue\n",
    "                        screen = j['screen']\n",
    "                        user = arr[0]\n",
    "                        m = x**2 + y**2 + z**2\n",
    "                        m = np.sqrt(m)\n",
    "                        ca = np.sqrt(y**2 + z**2)\n",
    "                        timestamp = arr[1]\n",
    "                        gyrSize =  gyrSize + 1\n",
    "                        df = {'x_gyroscope': x, 'y_gyroscope': y, 'z_gyroscope' : z, 'screen_gyroscope' : screen, 'user_gyroscope': user, 'magnitude_gyroscope' : m, 'combine_angle_gyroscope': ca, 'timestamp_gyroscope': timestamp}\n",
    "                        gyroscope = gyroscope.append(df, ignore_index=True)\n",
    "                    \n",
    "                dframe = {'accelometer_size': accSize, 'gyroscope_size': gyrSize, 'timestamp': arr[1]}\n",
    "                info = info.append(dframe, ignore_index=True)\n",
    "\n",
    "    return accelerometer, gyroscope, info, users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197c0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/SouthSystem/Documents/Pessoal/TCC/Impl/sensors_data'\n",
    "screen = 'MathisisGame'\n",
    "accelerometer, gyroscope, info, users = loadDataset(path, screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f37efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['x','y','z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f143646",
   "metadata": {},
   "outputs": [],
   "source": [
    "    user_list = list(sorted(set(accelerometer['user'])))\n",
    "    datadict = {}\n",
    "    user_y = []\n",
    "    finaldata = []\n",
    "    for user in user_list:\n",
    "        is_user = accelerometer['user']==user\n",
    "        accelerometer_user = accelerometer[is_user]    \n",
    "        filtered = accelerometer_user[['x','y','z','timestamp']]    \n",
    "        time_list = list(sorted(set(filtered['timestamp'])))\n",
    "        for idx, metric in enumerate(metrics):\n",
    "            data = []\n",
    "            for timestamp in time_list:\n",
    "                df = filtered[filtered['timestamp']==timestamp]            \n",
    "                list_df = list(df[metric].head(100))\n",
    "                if len(list_df) > 49:\n",
    "                    user_y.append(user)\n",
    "                    data.append(list_df)\n",
    "            datadict[idx] = data\n",
    "        data0 = datadict[0]\n",
    "        data1 = datadict[1]\n",
    "        data2 = datadict[2]\n",
    "        for idx, val in enumerate(data0):\n",
    "            finaldata.append(val + data1[idx] + data2[idx])\n",
    "\n",
    "    df = pd.DataFrame(finaldata)  \n",
    "    data_array = np.asarray(df)\n",
    "    user_ids = [user_list.index(user) for user in user_y]\n",
    "    user_array = np.asarray(user_ids)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26db91d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6    \\\n",
      "0    0.058594  0.065430  0.069336  0.079102  0.166016  0.107422  0.091797   \n",
      "1    0.023438  0.054688  0.021484 -0.011719 -0.055664  0.013672  0.001953   \n",
      "2    0.023438  0.054688  0.021484 -0.011719 -0.055664  0.013672  0.001953   \n",
      "3   -0.337891 -0.333008 -0.340820 -0.334961 -0.333984 -0.343750 -0.338867   \n",
      "4   -0.000977 -0.002930 -0.004883 -0.002930 -0.003906 -0.003906 -0.003906   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "355 -0.119263 -0.132935 -0.093873 -0.103638  0.005737 -0.074341 -0.082153   \n",
      "356 -0.179810 -0.201293 -0.072388 -0.037231  0.033081  0.076049  0.148316   \n",
      "357  0.083862  0.078004  0.078004  0.093629 -0.066528  0.060424  0.029174   \n",
      "358 -0.137829  0.000289  0.168484  0.195267  0.147812  0.160579  0.152421   \n",
      "359  0.162786  0.220567  0.190296  0.221627  0.255922  0.230804  0.148014   \n",
      "\n",
      "          7         8         9    ...       290       291       292  \\\n",
      "0    0.074219  0.000000  0.049805  ...  0.631836  0.638672  0.649414   \n",
      "1    0.044922  0.058594  0.032227  ...  0.510742  0.513672  0.477539   \n",
      "2    0.044922  0.058594  0.032227  ...  0.510742  0.513672  0.477539   \n",
      "3   -0.333984 -0.339844 -0.333008  ...  0.546875  0.563477  0.480469   \n",
      "4   -0.001953 -0.002930 -0.004883  ...  1.017578  1.017578  1.019531   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "355 -0.060669 -0.027466 -0.041138  ...  0.497314  0.499267  0.489501   \n",
      "356  0.054566  0.093629  0.103394  ...  0.530517  0.526611  0.538329   \n",
      "357  0.060424  0.066284  0.044799  ...  0.552001  0.552001  0.548096   \n",
      "358  0.174310  0.186078  0.145728  ...  0.913967  1.015257  0.920203   \n",
      "359  0.187108  0.224641  0.231243  ...  0.851526  0.843275  0.842196   \n",
      "\n",
      "          293       294       295       296       297       298       299  \n",
      "0    0.694336  0.653320  0.673828  0.637695  0.638672  0.518555  0.648438  \n",
      "1    0.625000  0.521484  0.554688  0.579102  0.580078  0.583984  0.585938  \n",
      "2    0.625000  0.521484  0.554688  0.579102  0.580078  0.583984  0.585938  \n",
      "3    0.543945  0.566406  0.544922  0.464844  0.779297  0.907227  0.601562  \n",
      "4    1.017578  1.018555  1.018555  1.017578  1.018555  1.017578  1.018555  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "355  0.475829  0.469971  0.479736  0.483642  0.479736  0.466064  0.444579  \n",
      "356  0.540283  0.532471  0.530517  0.528564  0.526611  0.503174  0.538329  \n",
      "357  0.553954  0.552001  0.546142  0.550049  0.555908  0.553954  0.553954  \n",
      "358  0.935509  0.901827  0.912733  0.915616  0.909669  0.976225  0.848145  \n",
      "359  0.839551  0.832386  0.851800  0.847063  0.849209  0.859338  0.844316  \n",
      "\n",
      "[360 rows x 300 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb62e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "X = None\n",
    "x_data = data_array\n",
    "for i in range(len(x_data)):\n",
    "    row = np.asarray(x_data[i, :])\n",
    "    row = row.reshape(3, 100).T\n",
    "    if X is None:\n",
    "        X = np.zeros((len(x_data), 100, 3))\n",
    "        X[i] = row\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b416a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(filtered['timestamp'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b54b40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial project configuration\n",
    "result = []\n",
    "project_name = 'Human Activity Recognition'\n",
    "arch = \"Convolution + pooling + convolution + pooling + dense + dense + dense + output\"\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "lr = 0.01\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a38b6456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19974c878f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "226d5d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = accelerometer.sample(frac=0.8, random_state=25)\n",
    "test_dataset = accelerometer.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a971e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(batch_size=64):\n",
    "    x_train, y_train, x_test, y_test = load_data()\n",
    "    x_train, x_test = x_train.reshape(\n",
    "        (-1, 3, 1, 100)), x_test.reshape((-1, 3, 1, 100))\n",
    "    transform = None\n",
    "    train_set = Data_loader(x_train, y_train, transform)\n",
    "    test_set = Data_loader(x_test, y_test, transform)\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    X_train, Y_train = format_data(train_dataset)\n",
    "    X_test, Y_test = format_data(test_dataset)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def format_data(dataitem):\n",
    "    user_list = list(sorted(set(dataitem['user'])))\n",
    "    datadict = {}\n",
    "    user_y = []\n",
    "    finaldata = []\n",
    "    for user in user_list:\n",
    "        is_user = dataitem['user']==user\n",
    "        accelerometer_user = dataitem[is_user]    \n",
    "        filtered = accelerometer_user[['x','y','z','timestamp']]    \n",
    "        time_list = list(sorted(set(filtered['timestamp'])))\n",
    "        for idx, metric in enumerate(metrics):\n",
    "            data = []\n",
    "            for timestamp in time_list:\n",
    "                df = filtered[filtered['timestamp']==timestamp]            \n",
    "                list_df = list(df[metric].head(100))\n",
    "                if len(list_df) > 49:\n",
    "                    user_y.append(user)\n",
    "                    data.append(list_df)\n",
    "            datadict[idx] = data\n",
    "        data0 = datadict[0]\n",
    "        data1 = datadict[1]\n",
    "        data2 = datadict[2]\n",
    "        for idx, val in enumerate(data0):\n",
    "            finaldata.append(val + data1[idx] + data2[idx])\n",
    "\n",
    "    df = pd.DataFrame(finaldata)  \n",
    "    data_array = np.asarray(df)\n",
    "    user_ids = [user_list.index(user) for user in user_y]\n",
    "    user_array = np.asarray(user_ids)\n",
    "    #print(data_array)\n",
    "    \n",
    "    X = None\n",
    "    x_data = data_array\n",
    "    for i in range(len(x_data)):\n",
    "        row = np.asarray(x_data[i, :])\n",
    "        row = row.reshape(3, 100).T\n",
    "        if X is None:\n",
    "            X = np.zeros((len(x_data), 100, 3))\n",
    "        X[i] = row\n",
    "    #print(X)\n",
    "    return X, user_array\n",
    "\n",
    "class Data_loader(Dataset):\n",
    "    def __init__(self, samples, labels, t):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.T = t\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample, target = self.samples[index], self.labels[index]\n",
    "        if self.T:\n",
    "            return self.T(sample), target\n",
    "        else:\n",
    "            return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40c94000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining network architecture\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=9, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=9, out_channels=64, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=2)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=64 * 23, out_features=400),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features=400, out_features=200),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(in_features=200, out_features=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(len(x))\n",
    "        out = self.conv1(x)\n",
    "        #print(out.shape)\n",
    "        out = self.conv2(out)\n",
    "        #print(out.shape)\n",
    "        out = out.reshape(-1, 64 * 23)\n",
    "        #print(len(out))\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ca80ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and plot functions\n",
    "def train(model, optimizer, train_loader, test_loader):\n",
    "    n_batch = len(train_loader.dataset) // batch_size\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        correct, total_loss = 0, 0\n",
    "        total = 0\n",
    "        for index, (sample, target) in enumerate(train_loader):\n",
    "            print(target)\n",
    "            sample, target = sample.to(\n",
    "                DEVICE).float(), target.to(DEVICE).long()\n",
    "            sample = sample.view(-1, 3, 1, 100)\n",
    "            print(target)\n",
    "            output = model(sample)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum()\n",
    "\n",
    "        acc_train = float(correct) * 100.0 / (batch_size * n_batch)\n",
    "        print(f'Epoch: [{e+1}/{epochs}], loss: {total_loss}, train acc: {acc_train}%')\n",
    "\n",
    "        # We proceed now to use the test data to evaluate intermediate results without modifying the model (no training)\n",
    "        model.train(False)\n",
    "        with torch.no_grad():\n",
    "            correct, total = 0, 0\n",
    "            for sample, target in test_loader:\n",
    "                sample, target = sample.to(\n",
    "                    DEVICE).float(), target.to(DEVICE).long()\n",
    "                sample = sample.view(-1, 3, 1, 100)\n",
    "                output = model(sample)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum()\n",
    "        acc_test = float(correct) * 100 / total\n",
    "        print(f'Epoch: [{e+1}/{epochs}], test acc: {float(correct) * 100 / total}%')\n",
    "        result.append([acc_train, acc_test])\n",
    "        result_np = np.array(result, dtype=float)\n",
    "        np.savetxt('result.csv', result_np, fmt='%.2f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "085fda8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get GPU if available\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "DEVICE = get_default_device()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0aa8f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load(\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91dc6326",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network().to(DEVICE)\n",
    "for index, (sample, target) in enumerate(train_loader):\n",
    "    sample, target = sample.to(    \n",
    "        DEVICE).float(), target.to(DEVICE).long() \n",
    "    sample = sample.view(-1, 3, 1, 100)\n",
    "    #print(len(sample))\n",
    "    output = model(sample)\n",
    "    #print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf6fd16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  4,  2,  7,  6,  4,  5,  9,  2,  0,  4,  2,  8,  0,  6,  9,  6,  4,\n",
      "         6,  4,  2,  0,  6,  6,  0,  3,  6,  6, 11,  9,  2,  1,  5,  2, 11, 11,\n",
      "         5, 10,  2,  2,  0,  5,  2,  7,  1,  2,  8,  6,  4,  5,  3,  9, 10, 10,\n",
      "         6,  2,  6,  4,  6,  6,  9,  8,  0,  3], dtype=torch.int32)\n",
      "tensor([10,  4,  2,  7,  6,  4,  5,  9,  2,  0,  4,  2,  8,  0,  6,  9,  6,  4,\n",
      "         6,  4,  2,  0,  6,  6,  0,  3,  6,  6, 11,  9,  2,  1,  5,  2, 11, 11,\n",
      "         5, 10,  2,  2,  0,  5,  2,  7,  1,  2,  8,  6,  4,  5,  3,  9, 10, 10,\n",
      "         6,  2,  6,  4,  6,  6,  9,  8,  0,  3])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 10 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SOUTHS~1\\AppData\\Local\\Temp/ipykernel_24952/3598320544.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SOUTHS~1\\AppData\\Local\\Temp/ipykernel_24952/839251467.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, test_loader)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\southsystem\\anaconda3\\envs\\fed-learning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\southsystem\\anaconda3\\envs\\fed-learning\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m    948\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\southsystem\\anaconda3\\envs\\fed-learning\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2422\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\southsystem\\anaconda3\\envs\\fed-learning\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2216\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   2217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2218\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2219\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2220\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 10 is out of bounds."
     ]
    }
   ],
   "source": [
    "# Load to selected device and traing model\n",
    "model = Network().to(DEVICE)\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=momentum)\n",
    "train(model, optimizer, train_loader, test_loader)\n",
    "result = np.array(result, dtype=float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
