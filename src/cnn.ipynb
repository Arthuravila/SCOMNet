{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85e35c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import scipy.stats as stats\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78d6c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "screens = ['Focus', 'Mathisis', 'Memoria', 'Reacton', 'Speedy']\n",
    "screens_code = ['1', '2', '3', '4', '5']\n",
    "\n",
    "base_path = \"\"\n",
    "phone_accel_file_paths = []\n",
    "phone_gyro_file_paths = []\n",
    "\n",
    "for directories, subdirectories, files in os.walk(base_path):\n",
    "    for filename in files:\n",
    "        if \"accel\" in filename:\n",
    "            phone_accel_file_paths.append(f\"{base_path}/accel/{filename}\")\n",
    "            \n",
    "data = pd.concat(map(pd.read_csv, phone_accel_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf6b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = data['player_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e5764422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_1():\n",
    "    train = np.empty((0, frame_size, 3))\n",
    "    user_list = []\n",
    "    frame_size = 500\n",
    "    step = 50\n",
    "\n",
    "    for user in users:\n",
    "        data_user = data[data['player_id']==user]  \n",
    "        data_user = data_user.iloc[:,[0,1,2]]\n",
    "        data_user = data_user[500:-500]\n",
    "        data_user = data_user.values\n",
    "        data_user = data_user.astype('float32')\n",
    "        frames = [data_user[i:i+frame_size, :] for i in range(0,data_user.shape[0]-frame_size,step)]\n",
    "        user_list.extend([user]*len(frames))           \n",
    "        frames = np.dstack(frames)\n",
    "        frames = np.rollaxis(frames,-1)\n",
    "        train = np.vstack((train, frames))\n",
    "        \n",
    "\n",
    "    return train, user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e20dc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_2():\n",
    "    train = []\n",
    "    frame_size = 500\n",
    "    step = 50\n",
    "\n",
    "    for user in users:\n",
    "        data_user = data[data['player_id']==user]  \n",
    "        data_user = data_user.iloc[:,[0,1,2]]\n",
    "        for w in range(0, data_user.shape[0] - frame_size, step):\n",
    "            end = w + frame_size        \n",
    "            frame = data_user.iloc[w:end,[0, 1, 2]]        \n",
    "            train.append(frame)\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "058fa9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_3():\n",
    "    data['session'] = data['player_id'] + \"_\" + data['timestamp'].apply(str)\n",
    "    \n",
    "    counts = data['session'].value_counts()\n",
    "    counts = counts[counts >= 128]\n",
    "    counts_list = list(counts.keys())\n",
    "    df = data[data.session.isin(counts_list) == True]\n",
    "    \n",
    "    for idx, val in enumerate(screens):\n",
    "        df.loc[df.screen.str.contains(screens[idx]), 'screen'] = screens_code[idx]\n",
    "        \n",
    "    win_count = 0\n",
    "    total_win_count = 0\n",
    "    range_screen = range(1, 6)\n",
    "    raw_signal = df\n",
    "    axis_list = ['x_accel', 'y_accel', 'z_accel']\n",
    "    user_list = []\n",
    "    window_size = 128\n",
    "    axis_dict = {}\n",
    "\n",
    "    for axis in axis_list:  \n",
    "        features_one = []\n",
    "        for class_label in range_screen:   \n",
    "            screen_ID = screens_code[class_label - 1]    \n",
    "            raw_data_one_activity = np.array(raw_signal.loc[raw_signal['screen'] == screen_ID, [axis]])\n",
    "            raw_data_one_activity = pd.DataFrame(raw_data_one_activity)   \n",
    "            player_id_data = np.array(raw_signal.loc[raw_signal['screen'] == screen_ID, ['player_id']])\n",
    "            player_id_data = pd.DataFrame(player_id_data)  \n",
    "\n",
    "            for data_point in range(0, len(raw_data_one_activity), window_size):        \n",
    "                win_count += 1\n",
    "                start = data_point\n",
    "                end = start + window_size\n",
    "                time_domain_window = raw_data_one_activity[start:end] \n",
    "\n",
    "                if (len(time_domain_window) == 128):                \n",
    "                    features_one.append(time_domain_window)\n",
    "                    if (axis == 'z_accel'):                    \n",
    "                        user_list.append(player_id_data[start:end][0].unique()[0])                    \n",
    "\n",
    "        axis_dict[axis] = features_one\n",
    "        \n",
    "    new = (axis_dict[axis_list[0]], axis_dict[axis_list[1]], axis_dict[axis_list[2]])\n",
    "    new_x = new[0]\n",
    "    new_x = np.asarray(new_x)\n",
    "    new_x = new_x.reshape(28473,-1)\n",
    "    print(new_x.shape)\n",
    "\n",
    "    new_y = new[1]\n",
    "    new_y = np.asarray(new_y)\n",
    "    new_y = new_y.reshape(28473,-1)\n",
    "    print(new_y.shape)\n",
    "\n",
    "    new_z = new[2]\n",
    "    new_z = np.asarray(new_z)\n",
    "    new_z = new_z.reshape(28473,-1)\n",
    "    print(new_z.shape)\n",
    "    \n",
    "    data_join = pd.DataFrame(np.concatenate((new_x, new_y, new_z), axis=1))\n",
    "    data_join['user'] = user_list\n",
    "    \n",
    "    return data_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32971e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(df):\n",
    "    array = df.values    \n",
    "    nsamples, nfeatures = array.shape\n",
    "    nfeatures = nfeatures - 1\n",
    "    X = array[:, 0:nfeatures]\n",
    "    y = array[:, -1]\n",
    "    \n",
    "    rows, cols = X.shape\n",
    "    \n",
    "    for i in range(0, rows):\n",
    "        row = X[i,:]\n",
    "        mu = np.mean( row )\n",
    "        sigma = np.std( row )\n",
    "        if( sigma == 0 ):\n",
    "            sigma = 0.0001\n",
    "        X[i,:] = (X[i,:] - mu) / sigma\n",
    "            \n",
    "    df = pd.DataFrame( X )\n",
    "    df['user'] = y \n",
    "    return df\n",
    "\n",
    "def unique(list1):       \n",
    "    list_set = set(list1) \n",
    "    unique_list = (list(list_set)) \n",
    "    unique_list.sort()\n",
    "    return unique_list\n",
    "\n",
    "def create_userids( df ):\n",
    "    array = df.values\n",
    "    y = array[:, -1]\n",
    "    return unique( y )\n",
    "\n",
    "def build_fcn(input_shape, nb_classes, file_path, num_filters = 128):\n",
    "    input_layer = keras.layers.Input(input_shape) \n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=num_filters, kernel_size=8, padding='same')(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=2*num_filters, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(num_filters, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics=['categorical_accuracy'])\n",
    "    learning_rate = 0.0001\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, \n",
    "                                                  min_lr=learning_rate)\n",
    "    \n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "    callbacks = [reduce_lr,model_checkpoint]\n",
    "\n",
    "    return callbacks, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6dd499f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 11235\n",
    "EPOCHS = 100\n",
    "\n",
    "def train_model(df, model_name = \"foo.h5\" ):\n",
    "    userids = create_userids( df )\n",
    "    nbclasses = len(userids)\n",
    "    print(nbclasses)\n",
    "    array = df.values\n",
    "    nsamples, nfeatures = array.shape\n",
    "    nfeatures = nfeatures -1 \n",
    "    X = array[:,0:nfeatures]\n",
    "    y = array[:,-1]\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y.reshape(-1,1))\n",
    "    y = enc.transform(y.reshape(-1, 1)).toarray()\n",
    "    X = X.reshape(-1, 128, 3)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RANDOM_STATE)\n",
    "    \n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(X_val.shape)\n",
    "    \n",
    "    mini_batch_size = int(min(X_train.shape[0]/10, 16))    \n",
    "    filepath = model_name\n",
    "    \n",
    "    cb, model = build_fcn((128, 3), nbclasses, filepath)\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    X_train = np.asarray(X_train).astype(np.float32)\n",
    "    X_val = np.asarray(X_val).astype(np.float32)\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "    BATCH_SIZE = mini_batch_size\n",
    "    SHUFFLE_BUFFER_SIZE = 100\n",
    "    \n",
    "    train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    val_ds = val_ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    hist = model.fit(train_ds, \n",
    "                      epochs=EPOCHS,\n",
    "                      verbose=True, \n",
    "                      validation_data=val_ds, \n",
    "                      callbacks=cb)\n",
    "    \n",
    "    hist_df = pd.DataFrame(hist.history) \n",
    "    \n",
    "    print(hist_df)\n",
    "    \n",
    "    plot_training(hist, model_name, metrics ='accuracy')\n",
    "    \n",
    "    hist_csv_file = 'history.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "    duration = time.time() - start_time\n",
    "    print(\"Training duration: \"+str(duration/60))\n",
    "    \n",
    "    # EVALUATION \n",
    "    X_test = np.asarray(X_test).astype(np.float32)    \n",
    "    y_true = np.argmax( y_test, axis=1)\n",
    "    y_pred = np.argmax( model.predict(X_test), axis=1)\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)     \n",
    "    print(accuracy)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plot_training(history, model_name, metrics ='loss'):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    keys = list(history.history.keys())\n",
    "    plt.figure()\n",
    "    if( metrics == 'loss'):\n",
    "        plt.plot(history.history[keys[0]])\n",
    "        plt.plot(history.history[keys[2]])\n",
    "        plt.title('Model loss ' + model_name)\n",
    "        plt.ylabel('loss')\n",
    "    \n",
    "    if( metrics == 'accuracy'):\n",
    "        plt.plot(history.history[keys[1]])\n",
    "        plt.plot(history.history[keys[3]])\n",
    "        plt.title('Model accuracy '+model_name)\n",
    "        plt.ylabel('accuracy')\n",
    "    \n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "606ce641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    \"\"\" Normalizes the data using StandardScaler() function \"\"\"\n",
    "    \n",
    "    data = data.drop([\"screen\", \"timestamp\"], axis = 1).copy()\n",
    "    \n",
    "    data.columns = ['X', 'Y', 'Z', 'User']\n",
    "        \n",
    "    le = LabelEncoder()\n",
    "    data['s'] = le.fit_transform(data['User'])\n",
    "    \n",
    "    X = data[['X', 'Y', 'Z']]\n",
    "    y = data['User']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    scaled_df = pd.DataFrame(data = X, columns = ['X', 'Y', 'Z'])\n",
    "    scaled_df['User'] = y.values\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "frequency = 100 # Based on Hertz\n",
    "time_period = 2 # Based on Second\n",
    "frame_size = frequency * time_period\n",
    "step_size = frame_size # In order not to have an overlap\n",
    "\n",
    "def get_frames(df):\n",
    "    n_features = 3\n",
    "    frames = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - frame_size, step_size):\n",
    "        x = df['X'].values[i: i + frame_size]\n",
    "        y = df['Y'].values[i: i + frame_size]\n",
    "        z = df['Z'].values[i: i + frame_size]\n",
    "        \n",
    "        label = stats.mode(df['User'][i: i + frame_size])[0][0]\n",
    "        frames.append([x, y, z])\n",
    "        labels.append(label)\n",
    "\n",
    "    frames = np.asarray(frames).reshape(-1, frame_size, n_features)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    print(frames.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    return frames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "862f8000",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SOUTHS~1\\AppData\\Local\\Temp/ipykernel_19672/1974796927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_set_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_list1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_set_join_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata_join\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set_join_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_join\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_list1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata_join\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data_1' is not defined"
     ]
    }
   ],
   "source": [
    "train_set_1, user_list1 = load_data_1()\n",
    "train_set_join_1 = train_set_1.reshape(train_set_1.shape[0], 1500)\n",
    "data_join = pd.DataFrame(train_set_join_1)\n",
    "data_join['user'] = user_list1\n",
    "data_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88cd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_2 = load_data_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16bafb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SouthSystem\\anaconda3\\envs\\python38\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28473, 128)\n",
      "(28473, 128)\n",
      "(28473, 128)\n"
     ]
    }
   ],
   "source": [
    "train_set_3 = load_data_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ade85be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.006350</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>-0.004396</td>\n",
       "      <td>-0.010257</td>\n",
       "      <td>-0.001954</td>\n",
       "      <td>0.008792</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>-0.008304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867484</td>\n",
       "      <td>0.850877</td>\n",
       "      <td>0.835735</td>\n",
       "      <td>0.837200</td>\n",
       "      <td>0.832315</td>\n",
       "      <td>0.830362</td>\n",
       "      <td>0.609095</td>\n",
       "      <td>0.806428</td>\n",
       "      <td>0.832804</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.049822</td>\n",
       "      <td>-0.054706</td>\n",
       "      <td>-0.033703</td>\n",
       "      <td>-0.025888</td>\n",
       "      <td>-0.017096</td>\n",
       "      <td>-0.021492</td>\n",
       "      <td>-0.046891</td>\n",
       "      <td>-0.011234</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.109412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773702</td>\n",
       "      <td>0.799589</td>\n",
       "      <td>0.766375</td>\n",
       "      <td>0.779563</td>\n",
       "      <td>0.774679</td>\n",
       "      <td>0.788355</td>\n",
       "      <td>0.816197</td>\n",
       "      <td>0.907537</td>\n",
       "      <td>0.810824</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038587</td>\n",
       "      <td>-0.091828</td>\n",
       "      <td>-0.264739</td>\n",
       "      <td>-0.301861</td>\n",
       "      <td>-0.128462</td>\n",
       "      <td>-0.054218</td>\n",
       "      <td>-0.057637</td>\n",
       "      <td>-0.064475</td>\n",
       "      <td>-0.074244</td>\n",
       "      <td>-0.046891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769794</td>\n",
       "      <td>0.784936</td>\n",
       "      <td>0.784936</td>\n",
       "      <td>0.785913</td>\n",
       "      <td>0.818639</td>\n",
       "      <td>0.750256</td>\n",
       "      <td>0.798124</td>\n",
       "      <td>0.771748</td>\n",
       "      <td>0.768817</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041030</td>\n",
       "      <td>-0.041030</td>\n",
       "      <td>-0.046403</td>\n",
       "      <td>-0.046403</td>\n",
       "      <td>-0.070825</td>\n",
       "      <td>-0.070825</td>\n",
       "      <td>-0.060079</td>\n",
       "      <td>-0.056172</td>\n",
       "      <td>-0.040053</td>\n",
       "      <td>-0.050799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793728</td>\n",
       "      <td>0.818150</td>\n",
       "      <td>0.811801</td>\n",
       "      <td>0.809358</td>\n",
       "      <td>0.818639</td>\n",
       "      <td>0.823035</td>\n",
       "      <td>0.820104</td>\n",
       "      <td>0.826454</td>\n",
       "      <td>0.790309</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.010257</td>\n",
       "      <td>-0.060568</td>\n",
       "      <td>-0.039564</td>\n",
       "      <td>-0.033703</td>\n",
       "      <td>-0.025888</td>\n",
       "      <td>-0.022957</td>\n",
       "      <td>-0.031261</td>\n",
       "      <td>-0.031261</td>\n",
       "      <td>-0.047379</td>\n",
       "      <td>-0.040053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716553</td>\n",
       "      <td>0.868949</td>\n",
       "      <td>0.783471</td>\n",
       "      <td>0.837688</td>\n",
       "      <td>0.769306</td>\n",
       "      <td>0.782005</td>\n",
       "      <td>0.767352</td>\n",
       "      <td>0.758560</td>\n",
       "      <td>0.781028</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28468</th>\n",
       "      <td>-0.024612</td>\n",
       "      <td>-0.057877</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>-0.051193</td>\n",
       "      <td>-0.011429</td>\n",
       "      <td>-0.035843</td>\n",
       "      <td>0.026520</td>\n",
       "      <td>-0.040070</td>\n",
       "      <td>-0.008011</td>\n",
       "      <td>-0.064087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.848969</td>\n",
       "      <td>-0.764481</td>\n",
       "      <td>-0.729523</td>\n",
       "      <td>-0.734634</td>\n",
       "      <td>-0.669601</td>\n",
       "      <td>-0.820999</td>\n",
       "      <td>-0.920471</td>\n",
       "      <td>-0.789948</td>\n",
       "      <td>-1.012207</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28469</th>\n",
       "      <td>-0.036652</td>\n",
       "      <td>0.145554</td>\n",
       "      <td>-0.092453</td>\n",
       "      <td>0.137421</td>\n",
       "      <td>-0.030548</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>-0.026505</td>\n",
       "      <td>-0.012405</td>\n",
       "      <td>-0.025391</td>\n",
       "      <td>-0.013855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.941116</td>\n",
       "      <td>-0.785400</td>\n",
       "      <td>-0.726425</td>\n",
       "      <td>-0.734833</td>\n",
       "      <td>-0.691284</td>\n",
       "      <td>-0.741684</td>\n",
       "      <td>-0.736221</td>\n",
       "      <td>-0.736526</td>\n",
       "      <td>-0.750732</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28470</th>\n",
       "      <td>-0.185852</td>\n",
       "      <td>-0.115402</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>-0.081787</td>\n",
       "      <td>0.020615</td>\n",
       "      <td>-0.145782</td>\n",
       "      <td>-0.043060</td>\n",
       "      <td>-0.202042</td>\n",
       "      <td>-0.073959</td>\n",
       "      <td>-0.100677</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788956</td>\n",
       "      <td>-0.934799</td>\n",
       "      <td>-1.065018</td>\n",
       "      <td>-0.849915</td>\n",
       "      <td>-0.890533</td>\n",
       "      <td>-0.766006</td>\n",
       "      <td>-0.966202</td>\n",
       "      <td>-1.179001</td>\n",
       "      <td>-0.820801</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28471</th>\n",
       "      <td>-0.075562</td>\n",
       "      <td>-0.045532</td>\n",
       "      <td>-0.145309</td>\n",
       "      <td>0.036514</td>\n",
       "      <td>-0.027328</td>\n",
       "      <td>-0.071823</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>0.123093</td>\n",
       "      <td>-0.103210</td>\n",
       "      <td>-0.108017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.971436</td>\n",
       "      <td>-0.955582</td>\n",
       "      <td>-1.138885</td>\n",
       "      <td>-0.921936</td>\n",
       "      <td>-0.955566</td>\n",
       "      <td>-0.930008</td>\n",
       "      <td>-0.504929</td>\n",
       "      <td>-0.501022</td>\n",
       "      <td>-0.935898</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28472</th>\n",
       "      <td>-0.042709</td>\n",
       "      <td>-0.049194</td>\n",
       "      <td>-0.051575</td>\n",
       "      <td>-0.057663</td>\n",
       "      <td>-0.044022</td>\n",
       "      <td>-0.030106</td>\n",
       "      <td>-0.024933</td>\n",
       "      <td>-0.041168</td>\n",
       "      <td>-0.033340</td>\n",
       "      <td>-0.066589</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.917114</td>\n",
       "      <td>-1.064514</td>\n",
       "      <td>-1.015472</td>\n",
       "      <td>-0.785675</td>\n",
       "      <td>-1.028290</td>\n",
       "      <td>-0.953857</td>\n",
       "      <td>-1.036377</td>\n",
       "      <td>-0.969177</td>\n",
       "      <td>-0.838623</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28473 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -0.006350  0.001954  0.002442  0.001465 -0.004396 -0.010257 -0.001954   \n",
       "1     -0.049822 -0.054706 -0.033703 -0.025888 -0.017096 -0.021492 -0.046891   \n",
       "2      0.038587 -0.091828 -0.264739 -0.301861 -0.128462 -0.054218 -0.057637   \n",
       "3     -0.041030 -0.041030 -0.046403 -0.046403 -0.070825 -0.070825 -0.060079   \n",
       "4     -0.010257 -0.060568 -0.039564 -0.033703 -0.025888 -0.022957 -0.031261   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "28468 -0.024612 -0.057877 -0.067642 -0.051193 -0.011429 -0.035843  0.026520   \n",
       "28469 -0.036652  0.145554 -0.092453  0.137421 -0.030548  0.004608 -0.026505   \n",
       "28470 -0.185852 -0.115402  0.009567 -0.081787  0.020615 -0.145782 -0.043060   \n",
       "28471 -0.075562 -0.045532 -0.145309  0.036514 -0.027328 -0.071823  0.012650   \n",
       "28472 -0.042709 -0.049194 -0.051575 -0.057663 -0.044022 -0.030106 -0.024933   \n",
       "\n",
       "              7         8         9  ...       375       376       377  \\\n",
       "0      0.008792  0.006350 -0.008304  ...  0.867484  0.850877  0.835735   \n",
       "1     -0.011234  0.004396  0.109412  ...  0.773702  0.799589  0.766375   \n",
       "2     -0.064475 -0.074244 -0.046891  ...  0.769794  0.784936  0.784936   \n",
       "3     -0.056172 -0.040053 -0.050799  ...  0.793728  0.818150  0.811801   \n",
       "4     -0.031261 -0.047379 -0.040053  ...  0.716553  0.868949  0.783471   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "28468 -0.040070 -0.008011 -0.064087  ... -0.848969 -0.764481 -0.729523   \n",
       "28469 -0.012405 -0.025391 -0.013855  ... -0.941116 -0.785400 -0.726425   \n",
       "28470 -0.202042 -0.073959 -0.100677  ... -0.788956 -0.934799 -1.065018   \n",
       "28471  0.123093 -0.103210 -0.108017  ... -0.971436 -0.955582 -1.138885   \n",
       "28472 -0.041168 -0.033340 -0.066589  ... -0.917114 -1.064514 -1.015472   \n",
       "\n",
       "            378       379       380       381       382       383     user  \n",
       "0      0.837200  0.832315  0.830362  0.609095  0.806428  0.832804  06mdn3c  \n",
       "1      0.779563  0.774679  0.788355  0.816197  0.907537  0.810824  06mdn3c  \n",
       "2      0.785913  0.818639  0.750256  0.798124  0.771748  0.768817  06mdn3c  \n",
       "3      0.809358  0.818639  0.823035  0.820104  0.826454  0.790309  06mdn3c  \n",
       "4      0.837688  0.769306  0.782005  0.767352  0.758560  0.781028  06mdn3c  \n",
       "...         ...       ...       ...       ...       ...       ...      ...  \n",
       "28468 -0.734634 -0.669601 -0.820999 -0.920471 -0.789948 -1.012207  x8rbf3x  \n",
       "28469 -0.734833 -0.691284 -0.741684 -0.736221 -0.736526 -0.750732  x8rbf3x  \n",
       "28470 -0.849915 -0.890533 -0.766006 -0.966202 -1.179001 -0.820801  x8rbf3x  \n",
       "28471 -0.921936 -0.955566 -0.930008 -0.504929 -0.501022 -0.935898  x8rbf3x  \n",
       "28472 -0.785675 -1.028290 -0.953857 -1.036377 -0.969177 -0.838623  x8rbf3x  \n",
       "\n",
       "[28473 rows x 385 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "044bbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_2 = np.asarray(train_set_2)\n",
    "train_set_join_2 = train_set_2.reshape(train_set_2.shape[0], 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47f2e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = scale_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b4a7675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20070, 200, 3)\n",
      "(20070,)\n"
     ]
    }
   ],
   "source": [
    "data_frames, labels = get_frames(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bce8df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20070, 601)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frames_join = data_frames.reshape(data_frames.shape[0], 600)\n",
    "data_join = pd.DataFrame(data_frames_join)\n",
    "data_join['user'] = labels\n",
    "data_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42a6e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idnet = data_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3c0acad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.293993</td>\n",
       "      <td>-1.270594</td>\n",
       "      <td>-1.269217</td>\n",
       "      <td>-1.27197</td>\n",
       "      <td>-1.288488</td>\n",
       "      <td>-1.305005</td>\n",
       "      <td>-1.281605</td>\n",
       "      <td>-1.251323</td>\n",
       "      <td>-1.258206</td>\n",
       "      <td>-1.299499</td>\n",
       "      <td>...</td>\n",
       "      <td>1.168471</td>\n",
       "      <td>1.121672</td>\n",
       "      <td>1.079002</td>\n",
       "      <td>1.083131</td>\n",
       "      <td>1.069367</td>\n",
       "      <td>1.063861</td>\n",
       "      <td>0.440331</td>\n",
       "      <td>0.996415</td>\n",
       "      <td>1.070744</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.594835</td>\n",
       "      <td>-1.609775</td>\n",
       "      <td>-1.545532</td>\n",
       "      <td>-1.521627</td>\n",
       "      <td>-1.494734</td>\n",
       "      <td>-1.508181</td>\n",
       "      <td>-1.58587</td>\n",
       "      <td>-1.476806</td>\n",
       "      <td>-1.428997</td>\n",
       "      <td>-1.10778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924107</td>\n",
       "      <td>1.00329</td>\n",
       "      <td>0.901696</td>\n",
       "      <td>0.942035</td>\n",
       "      <td>0.927095</td>\n",
       "      <td>0.968928</td>\n",
       "      <td>1.054088</td>\n",
       "      <td>1.333472</td>\n",
       "      <td>1.037653</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.178501</td>\n",
       "      <td>-1.5472</td>\n",
       "      <td>-2.036037</td>\n",
       "      <td>-2.140985</td>\n",
       "      <td>-1.650767</td>\n",
       "      <td>-1.440871</td>\n",
       "      <td>-1.450538</td>\n",
       "      <td>-1.46987</td>\n",
       "      <td>-1.497488</td>\n",
       "      <td>-1.420158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8887</td>\n",
       "      <td>0.931508</td>\n",
       "      <td>0.931508</td>\n",
       "      <td>0.93427</td>\n",
       "      <td>1.026789</td>\n",
       "      <td>0.833464</td>\n",
       "      <td>0.968792</td>\n",
       "      <td>0.894224</td>\n",
       "      <td>0.885938</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.409391</td>\n",
       "      <td>-1.409391</td>\n",
       "      <td>-1.424615</td>\n",
       "      <td>-1.424615</td>\n",
       "      <td>-1.493812</td>\n",
       "      <td>-1.493812</td>\n",
       "      <td>-1.463365</td>\n",
       "      <td>-1.452294</td>\n",
       "      <td>-1.406623</td>\n",
       "      <td>-1.43707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955774</td>\n",
       "      <td>1.024971</td>\n",
       "      <td>1.00698</td>\n",
       "      <td>1.00006</td>\n",
       "      <td>1.026355</td>\n",
       "      <td>1.038811</td>\n",
       "      <td>1.030507</td>\n",
       "      <td>1.048498</td>\n",
       "      <td>0.946086</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.280293</td>\n",
       "      <td>-1.419168</td>\n",
       "      <td>-1.361191</td>\n",
       "      <td>-1.345011</td>\n",
       "      <td>-1.323439</td>\n",
       "      <td>-1.315349</td>\n",
       "      <td>-1.33827</td>\n",
       "      <td>-1.33827</td>\n",
       "      <td>-1.382764</td>\n",
       "      <td>-1.362539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.72598</td>\n",
       "      <td>1.14665</td>\n",
       "      <td>0.910697</td>\n",
       "      <td>1.060358</td>\n",
       "      <td>0.871596</td>\n",
       "      <td>0.906652</td>\n",
       "      <td>0.866203</td>\n",
       "      <td>0.841933</td>\n",
       "      <td>0.903955</td>\n",
       "      <td>06mdn3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28468</th>\n",
       "      <td>1.300156</td>\n",
       "      <td>1.20367</td>\n",
       "      <td>1.175343</td>\n",
       "      <td>1.223055</td>\n",
       "      <td>1.338397</td>\n",
       "      <td>1.267581</td>\n",
       "      <td>1.448471</td>\n",
       "      <td>1.255321</td>\n",
       "      <td>1.348311</td>\n",
       "      <td>1.185656</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.090988</td>\n",
       "      <td>-0.84592</td>\n",
       "      <td>-0.744521</td>\n",
       "      <td>-0.759348</td>\n",
       "      <td>-0.570712</td>\n",
       "      <td>-1.009859</td>\n",
       "      <td>-1.29839</td>\n",
       "      <td>-0.91979</td>\n",
       "      <td>-1.564481</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28469</th>\n",
       "      <td>1.3894</td>\n",
       "      <td>1.958652</td>\n",
       "      <td>1.215063</td>\n",
       "      <td>1.933243</td>\n",
       "      <td>1.408469</td>\n",
       "      <td>1.518305</td>\n",
       "      <td>1.421102</td>\n",
       "      <td>1.465151</td>\n",
       "      <td>1.424582</td>\n",
       "      <td>1.460622</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.436361</td>\n",
       "      <td>-0.949868</td>\n",
       "      <td>-0.765615</td>\n",
       "      <td>-0.791883</td>\n",
       "      <td>-0.655827</td>\n",
       "      <td>-0.813287</td>\n",
       "      <td>-0.796221</td>\n",
       "      <td>-0.797174</td>\n",
       "      <td>-0.841557</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28470</th>\n",
       "      <td>0.923591</td>\n",
       "      <td>1.140215</td>\n",
       "      <td>1.524479</td>\n",
       "      <td>1.243577</td>\n",
       "      <td>1.558449</td>\n",
       "      <td>1.046799</td>\n",
       "      <td>1.362657</td>\n",
       "      <td>0.87381</td>\n",
       "      <td>1.267646</td>\n",
       "      <td>1.185491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930874</td>\n",
       "      <td>-1.379324</td>\n",
       "      <td>-1.779729</td>\n",
       "      <td>-1.118315</td>\n",
       "      <td>-1.243212</td>\n",
       "      <td>-0.860308</td>\n",
       "      <td>-1.475883</td>\n",
       "      <td>-2.130212</td>\n",
       "      <td>-1.028794</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28471</th>\n",
       "      <td>1.129768</td>\n",
       "      <td>1.217355</td>\n",
       "      <td>0.926332</td>\n",
       "      <td>1.456663</td>\n",
       "      <td>1.270451</td>\n",
       "      <td>1.140672</td>\n",
       "      <td>1.387056</td>\n",
       "      <td>1.709188</td>\n",
       "      <td>1.049124</td>\n",
       "      <td>1.035104</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.483254</td>\n",
       "      <td>-1.437012</td>\n",
       "      <td>-1.97166</td>\n",
       "      <td>-1.338877</td>\n",
       "      <td>-1.436968</td>\n",
       "      <td>-1.362421</td>\n",
       "      <td>-0.12258</td>\n",
       "      <td>-0.111186</td>\n",
       "      <td>-1.3796</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28472</th>\n",
       "      <td>1.202386</td>\n",
       "      <td>1.18373</td>\n",
       "      <td>1.176882</td>\n",
       "      <td>1.159366</td>\n",
       "      <td>1.198611</td>\n",
       "      <td>1.238645</td>\n",
       "      <td>1.253527</td>\n",
       "      <td>1.20682</td>\n",
       "      <td>1.229339</td>\n",
       "      <td>1.133686</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.31316</td>\n",
       "      <td>-1.73721</td>\n",
       "      <td>-1.596123</td>\n",
       "      <td>-0.935027</td>\n",
       "      <td>-1.632997</td>\n",
       "      <td>-1.418865</td>\n",
       "      <td>-1.656263</td>\n",
       "      <td>-1.462938</td>\n",
       "      <td>-1.087351</td>\n",
       "      <td>x8rbf3x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28473 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -1.293993 -1.270594 -1.269217  -1.27197 -1.288488 -1.305005 -1.281605   \n",
       "1     -1.594835 -1.609775 -1.545532 -1.521627 -1.494734 -1.508181  -1.58587   \n",
       "2     -1.178501   -1.5472 -2.036037 -2.140985 -1.650767 -1.440871 -1.450538   \n",
       "3     -1.409391 -1.409391 -1.424615 -1.424615 -1.493812 -1.493812 -1.463365   \n",
       "4     -1.280293 -1.419168 -1.361191 -1.345011 -1.323439 -1.315349  -1.33827   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "28468  1.300156   1.20367  1.175343  1.223055  1.338397  1.267581  1.448471   \n",
       "28469    1.3894  1.958652  1.215063  1.933243  1.408469  1.518305  1.421102   \n",
       "28470  0.923591  1.140215  1.524479  1.243577  1.558449  1.046799  1.362657   \n",
       "28471  1.129768  1.217355  0.926332  1.456663  1.270451  1.140672  1.387056   \n",
       "28472  1.202386   1.18373  1.176882  1.159366  1.198611  1.238645  1.253527   \n",
       "\n",
       "              7         8         9  ...       375       376       377  \\\n",
       "0     -1.251323 -1.258206 -1.299499  ...  1.168471  1.121672  1.079002   \n",
       "1     -1.476806 -1.428997  -1.10778  ...  0.924107   1.00329  0.901696   \n",
       "2      -1.46987 -1.497488 -1.420158  ...    0.8887  0.931508  0.931508   \n",
       "3     -1.452294 -1.406623  -1.43707  ...  0.955774  1.024971   1.00698   \n",
       "4      -1.33827 -1.382764 -1.362539  ...   0.72598   1.14665  0.910697   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "28468  1.255321  1.348311  1.185656  ... -1.090988  -0.84592 -0.744521   \n",
       "28469  1.465151  1.424582  1.460622  ... -1.436361 -0.949868 -0.765615   \n",
       "28470   0.87381  1.267646  1.185491  ... -0.930874 -1.379324 -1.779729   \n",
       "28471  1.709188  1.049124  1.035104  ... -1.483254 -1.437012  -1.97166   \n",
       "28472   1.20682  1.229339  1.133686  ...  -1.31316  -1.73721 -1.596123   \n",
       "\n",
       "            378       379       380       381       382       383     user  \n",
       "0      1.083131  1.069367  1.063861  0.440331  0.996415  1.070744  06mdn3c  \n",
       "1      0.942035  0.927095  0.968928  1.054088  1.333472  1.037653  06mdn3c  \n",
       "2       0.93427  1.026789  0.833464  0.968792  0.894224  0.885938  06mdn3c  \n",
       "3       1.00006  1.026355  1.038811  1.030507  1.048498  0.946086  06mdn3c  \n",
       "4      1.060358  0.871596  0.906652  0.866203  0.841933  0.903955  06mdn3c  \n",
       "...         ...       ...       ...       ...       ...       ...      ...  \n",
       "28468 -0.759348 -0.570712 -1.009859  -1.29839  -0.91979 -1.564481  x8rbf3x  \n",
       "28469 -0.791883 -0.655827 -0.813287 -0.796221 -0.797174 -0.841557  x8rbf3x  \n",
       "28470 -1.118315 -1.243212 -0.860308 -1.475883 -2.130212 -1.028794  x8rbf3x  \n",
       "28471 -1.338877 -1.436968 -1.362421  -0.12258 -0.111186   -1.3796  x8rbf3x  \n",
       "28472 -0.935027 -1.632997 -1.418865 -1.656263 -1.462938 -1.087351  x8rbf3x  \n",
       "\n",
       "[28473 rows x 385 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idnet = normalize_rows(train_set_3)\n",
    "df_idnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "411034cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "(17083, 128, 3)\n",
      "(5695, 128, 3)\n",
      "(5695, 128, 3)\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128, 3)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 128, 128)          3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 128, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 128, 128)          98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                3870      \n",
      "=================================================================\n",
      "Total params: 271,646\n",
      "Trainable params: 270,622\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 2.1177 - categorical_accuracy: 0.4143\n",
      "Epoch 00001: val_loss improved from inf to 1.94949, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 41s 39ms/step - loss: 2.1178 - categorical_accuracy: 0.4143 - val_loss: 1.9495 - val_categorical_accuracy: 0.4212\n",
      "Epoch 2/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.8324 - categorical_accuracy: 0.4610\n",
      "Epoch 00002: val_loss did not improve from 1.94949\n",
      "1068/1068 [==============================] - 43s 40ms/step - loss: 1.8324 - categorical_accuracy: 0.4610 - val_loss: 1.9537 - val_categorical_accuracy: 0.4615\n",
      "Epoch 3/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.7380 - categorical_accuracy: 0.4778\n",
      "Epoch 00003: val_loss improved from 1.94949 to 1.77456, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.7380 - categorical_accuracy: 0.4778 - val_loss: 1.7746 - val_categorical_accuracy: 0.4815\n",
      "Epoch 4/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.6841 - categorical_accuracy: 0.4923\n",
      "Epoch 00004: val_loss improved from 1.77456 to 1.72633, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 45s 42ms/step - loss: 1.6840 - categorical_accuracy: 0.4923 - val_loss: 1.7263 - val_categorical_accuracy: 0.4878\n",
      "Epoch 5/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.6407 - categorical_accuracy: 0.5010\n",
      "Epoch 00005: val_loss did not improve from 1.72633\n",
      "1068/1068 [==============================] - 52s 49ms/step - loss: 1.6410 - categorical_accuracy: 0.5010 - val_loss: 1.8034 - val_categorical_accuracy: 0.4478\n",
      "Epoch 6/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.6149 - categorical_accuracy: 0.5090\n",
      "Epoch 00006: val_loss improved from 1.72633 to 1.72472, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 49s 46ms/step - loss: 1.6149 - categorical_accuracy: 0.5090 - val_loss: 1.7247 - val_categorical_accuracy: 0.4852\n",
      "Epoch 7/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.5890 - categorical_accuracy: 0.5128\n",
      "Epoch 00007: val_loss did not improve from 1.72472\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 1.5888 - categorical_accuracy: 0.5128 - val_loss: 1.9413 - val_categorical_accuracy: 0.4197\n",
      "Epoch 8/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.5662 - categorical_accuracy: 0.5224\n",
      "Epoch 00008: val_loss improved from 1.72472 to 1.69590, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.5662 - categorical_accuracy: 0.5224 - val_loss: 1.6959 - val_categorical_accuracy: 0.4802\n",
      "Epoch 9/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.5429 - categorical_accuracy: 0.5261\n",
      "Epoch 00009: val_loss did not improve from 1.69590\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.5429 - categorical_accuracy: 0.5261 - val_loss: 1.7484 - val_categorical_accuracy: 0.4969\n",
      "Epoch 10/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.5236 - categorical_accuracy: 0.5324\n",
      "Epoch 00010: val_loss did not improve from 1.69590\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 1.5236 - categorical_accuracy: 0.5324 - val_loss: 2.2904 - val_categorical_accuracy: 0.3993\n",
      "Epoch 11/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.4992 - categorical_accuracy: 0.5354\n",
      "Epoch 00011: val_loss did not improve from 1.69590\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.4992 - categorical_accuracy: 0.5353 - val_loss: 1.7434 - val_categorical_accuracy: 0.4562\n",
      "Epoch 12/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.4826 - categorical_accuracy: 0.5408\n",
      "Epoch 00012: val_loss improved from 1.69590 to 1.60135, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 45s 42ms/step - loss: 1.4828 - categorical_accuracy: 0.5407 - val_loss: 1.6014 - val_categorical_accuracy: 0.4825\n",
      "Epoch 13/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.4686 - categorical_accuracy: 0.5445\n",
      "Epoch 00013: val_loss did not improve from 1.60135\n",
      "1068/1068 [==============================] - 43s 40ms/step - loss: 1.4686 - categorical_accuracy: 0.5445 - val_loss: 2.0755 - val_categorical_accuracy: 0.3860\n",
      "Epoch 14/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.4532 - categorical_accuracy: 0.5486\n",
      "Epoch 00014: val_loss did not improve from 1.60135\n",
      "1068/1068 [==============================] - 44s 41ms/step - loss: 1.4530 - categorical_accuracy: 0.5487 - val_loss: 2.0838 - val_categorical_accuracy: 0.4090\n",
      "Epoch 15/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.4307 - categorical_accuracy: 0.5527\n",
      "Epoch 00015: val_loss improved from 1.60135 to 1.60040, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 45s 42ms/step - loss: 1.4307 - categorical_accuracy: 0.5527 - val_loss: 1.6004 - val_categorical_accuracy: 0.5241\n",
      "Epoch 16/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.4161 - categorical_accuracy: 0.5565\n",
      "Epoch 00016: val_loss improved from 1.60040 to 1.55206, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 46s 43ms/step - loss: 1.4160 - categorical_accuracy: 0.5566 - val_loss: 1.5521 - val_categorical_accuracy: 0.5245\n",
      "Epoch 17/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.3979 - categorical_accuracy: 0.5637\n",
      "Epoch 00017: val_loss improved from 1.55206 to 1.54859, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 1.3977 - categorical_accuracy: 0.5638 - val_loss: 1.5486 - val_categorical_accuracy: 0.5034\n",
      "Epoch 18/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.3831 - categorical_accuracy: 0.5666\n",
      "Epoch 00018: val_loss did not improve from 1.54859\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 1.3831 - categorical_accuracy: 0.5666 - val_loss: 1.5693 - val_categorical_accuracy: 0.5064\n",
      "Epoch 19/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.3727 - categorical_accuracy: 0.5665\n",
      "Epoch 00019: val_loss improved from 1.54859 to 1.47500, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 46s 43ms/step - loss: 1.3727 - categorical_accuracy: 0.5665 - val_loss: 1.4750 - val_categorical_accuracy: 0.5370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.3567 - categorical_accuracy: 0.5750\n",
      "Epoch 00020: val_loss did not improve from 1.47500\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 1.3567 - categorical_accuracy: 0.5750 - val_loss: 1.5454 - val_categorical_accuracy: 0.5173\n",
      "Epoch 21/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.3528 - categorical_accuracy: 0.5748\n",
      "Epoch 00021: val_loss did not improve from 1.47500\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 1.3530 - categorical_accuracy: 0.5748 - val_loss: 1.5157 - val_categorical_accuracy: 0.5417\n",
      "Epoch 22/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.3368 - categorical_accuracy: 0.5778\n",
      "Epoch 00022: val_loss improved from 1.47500 to 1.43751, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 46s 43ms/step - loss: 1.3368 - categorical_accuracy: 0.5778 - val_loss: 1.4375 - val_categorical_accuracy: 0.5528\n",
      "Epoch 23/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.3227 - categorical_accuracy: 0.5829\n",
      "Epoch 00023: val_loss did not improve from 1.43751\n",
      "1068/1068 [==============================] - 46s 43ms/step - loss: 1.3228 - categorical_accuracy: 0.5829 - val_loss: 1.5971 - val_categorical_accuracy: 0.5450\n",
      "Epoch 24/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.3074 - categorical_accuracy: 0.5915\n",
      "Epoch 00024: val_loss did not improve from 1.43751\n",
      "1068/1068 [==============================] - 50s 47ms/step - loss: 1.3074 - categorical_accuracy: 0.5915 - val_loss: 1.8482 - val_categorical_accuracy: 0.4290\n",
      "Epoch 25/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.2965 - categorical_accuracy: 0.5913\n",
      "Epoch 00025: val_loss did not improve from 1.43751\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.2967 - categorical_accuracy: 0.5913 - val_loss: 1.5184 - val_categorical_accuracy: 0.5166\n",
      "Epoch 26/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.2884 - categorical_accuracy: 0.5912\n",
      "Epoch 00026: val_loss did not improve from 1.43751\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.2885 - categorical_accuracy: 0.5912 - val_loss: 1.4795 - val_categorical_accuracy: 0.5543\n",
      "Epoch 27/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.2713 - categorical_accuracy: 0.6001\n",
      "Epoch 00027: val_loss did not improve from 1.43751\n",
      "1068/1068 [==============================] - 49s 46ms/step - loss: 1.2713 - categorical_accuracy: 0.6001 - val_loss: 1.8188 - val_categorical_accuracy: 0.4674\n",
      "Epoch 28/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.2603 - categorical_accuracy: 0.6022\n",
      "Epoch 00028: val_loss did not improve from 1.43751\n",
      "1068/1068 [==============================] - 49s 46ms/step - loss: 1.2604 - categorical_accuracy: 0.6022 - val_loss: 1.6587 - val_categorical_accuracy: 0.4790\n",
      "Epoch 29/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.2502 - categorical_accuracy: 0.6042\n",
      "Epoch 00029: val_loss did not improve from 1.43751\n",
      "1068/1068 [==============================] - 49s 46ms/step - loss: 1.2502 - categorical_accuracy: 0.6042 - val_loss: 1.4402 - val_categorical_accuracy: 0.5666\n",
      "Epoch 30/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.2461 - categorical_accuracy: 0.6038\n",
      "Epoch 00030: val_loss improved from 1.43751 to 1.35158, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 49s 46ms/step - loss: 1.2461 - categorical_accuracy: 0.6038 - val_loss: 1.3516 - val_categorical_accuracy: 0.5761\n",
      "Epoch 31/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.2321 - categorical_accuracy: 0.6088\n",
      "Epoch 00031: val_loss did not improve from 1.35158\n",
      "1068/1068 [==============================] - 50s 47ms/step - loss: 1.2325 - categorical_accuracy: 0.6087 - val_loss: 1.6653 - val_categorical_accuracy: 0.4973\n",
      "Epoch 32/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.2191 - categorical_accuracy: 0.6141\n",
      "Epoch 00032: val_loss did not improve from 1.35158\n",
      "1068/1068 [==============================] - 49s 46ms/step - loss: 1.2191 - categorical_accuracy: 0.6141 - val_loss: 1.6104 - val_categorical_accuracy: 0.5104\n",
      "Epoch 33/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.2123 - categorical_accuracy: 0.6178\n",
      "Epoch 00033: val_loss did not improve from 1.35158\n",
      "1068/1068 [==============================] - 50s 47ms/step - loss: 1.2122 - categorical_accuracy: 0.6178 - val_loss: 1.4248 - val_categorical_accuracy: 0.5719\n",
      "Epoch 34/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.1992 - categorical_accuracy: 0.6177\n",
      "Epoch 00034: val_loss did not improve from 1.35158\n",
      "1068/1068 [==============================] - 50s 46ms/step - loss: 1.1992 - categorical_accuracy: 0.6177 - val_loss: 1.5069 - val_categorical_accuracy: 0.5282\n",
      "Epoch 35/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.1873 - categorical_accuracy: 0.6256\n",
      "Epoch 00035: val_loss did not improve from 1.35158\n",
      "1068/1068 [==============================] - 50s 47ms/step - loss: 1.1873 - categorical_accuracy: 0.6256 - val_loss: 1.4674 - val_categorical_accuracy: 0.5594\n",
      "Epoch 36/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.1860 - categorical_accuracy: 0.6232\n",
      "Epoch 00036: val_loss did not improve from 1.35158\n",
      "1068/1068 [==============================] - 49s 46ms/step - loss: 1.1858 - categorical_accuracy: 0.6233 - val_loss: 1.3839 - val_categorical_accuracy: 0.5910\n",
      "Epoch 37/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.1657 - categorical_accuracy: 0.6313\n",
      "Epoch 00037: val_loss did not improve from 1.35158\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 1.1657 - categorical_accuracy: 0.6313 - val_loss: 1.3585 - val_categorical_accuracy: 0.5775\n",
      "Epoch 38/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.1644 - categorical_accuracy: 0.6299\n",
      "Epoch 00038: val_loss improved from 1.35158 to 1.30867, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 51s 48ms/step - loss: 1.1646 - categorical_accuracy: 0.6299 - val_loss: 1.3087 - val_categorical_accuracy: 0.5949\n",
      "Epoch 39/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.1546 - categorical_accuracy: 0.6334\n",
      "Epoch 00039: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 51s 48ms/step - loss: 1.1546 - categorical_accuracy: 0.6334 - val_loss: 1.3518 - val_categorical_accuracy: 0.5831\n",
      "Epoch 40/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.1374 - categorical_accuracy: 0.6391\n",
      "Epoch 00040: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 51s 48ms/step - loss: 1.1372 - categorical_accuracy: 0.6392 - val_loss: 2.3249 - val_categorical_accuracy: 0.3930\n",
      "Epoch 41/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.1328 - categorical_accuracy: 0.6369\n",
      "Epoch 00041: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 50s 47ms/step - loss: 1.1329 - categorical_accuracy: 0.6368 - val_loss: 1.8102 - val_categorical_accuracy: 0.4311\n",
      "Epoch 42/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.1183 - categorical_accuracy: 0.6417\n",
      "Epoch 00042: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.1183 - categorical_accuracy: 0.6417 - val_loss: 1.5852 - val_categorical_accuracy: 0.5399\n",
      "Epoch 43/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.1196 - categorical_accuracy: 0.6447\n",
      "Epoch 00043: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 49s 46ms/step - loss: 1.1199 - categorical_accuracy: 0.6446 - val_loss: 1.3141 - val_categorical_accuracy: 0.5821\n",
      "Epoch 44/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.1089 - categorical_accuracy: 0.6474\n",
      "Epoch 00044: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.1089 - categorical_accuracy: 0.6474 - val_loss: 1.3942 - val_categorical_accuracy: 0.5709\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068/1068 [==============================] - ETA: 0s - loss: 1.1071 - categorical_accuracy: 0.6477\n",
      "Epoch 00045: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 46s 43ms/step - loss: 1.1071 - categorical_accuracy: 0.6477 - val_loss: 1.5952 - val_categorical_accuracy: 0.5403\n",
      "Epoch 46/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.0988 - categorical_accuracy: 0.6520\n",
      "Epoch 00046: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 46s 43ms/step - loss: 1.0989 - categorical_accuracy: 0.6518 - val_loss: 1.6797 - val_categorical_accuracy: 0.4848\n",
      "Epoch 47/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.0827 - categorical_accuracy: 0.6556\n",
      "Epoch 00047: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.0827 - categorical_accuracy: 0.6556 - val_loss: 1.3703 - val_categorical_accuracy: 0.5951\n",
      "Epoch 48/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.0686 - categorical_accuracy: 0.6563\n",
      "Epoch 00048: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 1.0684 - categorical_accuracy: 0.6564 - val_loss: 1.5389 - val_categorical_accuracy: 0.5259\n",
      "Epoch 49/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.0663 - categorical_accuracy: 0.6618\n",
      "Epoch 00049: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 1.0665 - categorical_accuracy: 0.6617 - val_loss: 1.3201 - val_categorical_accuracy: 0.5889\n",
      "Epoch 50/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.0598 - categorical_accuracy: 0.6617\n",
      "Epoch 00050: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.0598 - categorical_accuracy: 0.6617 - val_loss: 1.4525 - val_categorical_accuracy: 0.5572\n",
      "Epoch 51/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.0573 - categorical_accuracy: 0.6617\n",
      "Epoch 00051: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.0573 - categorical_accuracy: 0.6617 - val_loss: 1.6480 - val_categorical_accuracy: 0.5083\n",
      "Epoch 52/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.0405 - categorical_accuracy: 0.6714\n",
      "Epoch 00052: val_loss did not improve from 1.30867\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 1.0405 - categorical_accuracy: 0.6714 - val_loss: 1.4920 - val_categorical_accuracy: 0.5763\n",
      "Epoch 53/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.0379 - categorical_accuracy: 0.6702\n",
      "Epoch 00053: val_loss improved from 1.30867 to 1.26610, saving model to gait_fcn.h5\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.0381 - categorical_accuracy: 0.6701 - val_loss: 1.2661 - val_categorical_accuracy: 0.6195\n",
      "Epoch 54/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.0267 - categorical_accuracy: 0.6694\n",
      "Epoch 00054: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 49s 46ms/step - loss: 1.0267 - categorical_accuracy: 0.6694 - val_loss: 2.1272 - val_categorical_accuracy: 0.4541\n",
      "Epoch 55/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.0189 - categorical_accuracy: 0.6760\n",
      "Epoch 00055: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 46s 43ms/step - loss: 1.0188 - categorical_accuracy: 0.6759 - val_loss: 2.4246 - val_categorical_accuracy: 0.4021\n",
      "Epoch 56/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 1.0143 - categorical_accuracy: 0.6731\n",
      "Epoch 00056: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 1.0142 - categorical_accuracy: 0.6731 - val_loss: 1.4632 - val_categorical_accuracy: 0.5724\n",
      "Epoch 57/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 1.0093 - categorical_accuracy: 0.6772\n",
      "Epoch 00057: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 1.0093 - categorical_accuracy: 0.6772 - val_loss: 1.6739 - val_categorical_accuracy: 0.5296\n",
      "Epoch 58/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.9962 - categorical_accuracy: 0.6795\n",
      "Epoch 00058: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 44ms/step - loss: 0.9962 - categorical_accuracy: 0.6795 - val_loss: 1.4655 - val_categorical_accuracy: 0.5665\n",
      "Epoch 59/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.9914 - categorical_accuracy: 0.6843\n",
      "Epoch 00059: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.9912 - categorical_accuracy: 0.6844 - val_loss: 1.4169 - val_categorical_accuracy: 0.5765\n",
      "Epoch 60/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.9794 - categorical_accuracy: 0.6873\n",
      "Epoch 00060: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.9794 - categorical_accuracy: 0.6873 - val_loss: 1.3416 - val_categorical_accuracy: 0.5939\n",
      "Epoch 61/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.9712 - categorical_accuracy: 0.6895\n",
      "Epoch 00061: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 0.9709 - categorical_accuracy: 0.6896 - val_loss: 2.1862 - val_categorical_accuracy: 0.3837\n",
      "Epoch 62/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.9648 - categorical_accuracy: 0.6916 ETA: 1s - loss: 0.9682 \n",
      "Epoch 00062: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 46s 43ms/step - loss: 0.9646 - categorical_accuracy: 0.6916 - val_loss: 1.5619 - val_categorical_accuracy: 0.5384\n",
      "Epoch 63/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.9594 - categorical_accuracy: 0.6892\n",
      "Epoch 00063: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 0.9594 - categorical_accuracy: 0.6892 - val_loss: 1.2664 - val_categorical_accuracy: 0.6277\n",
      "Epoch 64/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.9483 - categorical_accuracy: 0.6963\n",
      "Epoch 00064: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.9483 - categorical_accuracy: 0.6963 - val_loss: 1.9046 - val_categorical_accuracy: 0.4379\n",
      "Epoch 65/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.9467 - categorical_accuracy: 0.6976\n",
      "Epoch 00065: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 50s 47ms/step - loss: 0.9472 - categorical_accuracy: 0.6975 - val_loss: 1.2956 - val_categorical_accuracy: 0.6140\n",
      "Epoch 66/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.9339 - categorical_accuracy: 0.7009\n",
      "Epoch 00066: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 55s 52ms/step - loss: 0.9339 - categorical_accuracy: 0.7009 - val_loss: 1.6919 - val_categorical_accuracy: 0.5112\n",
      "Epoch 67/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.9278 - categorical_accuracy: 0.7038\n",
      "Epoch 00067: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 54s 50ms/step - loss: 0.9275 - categorical_accuracy: 0.7039 - val_loss: 1.7191 - val_categorical_accuracy: 0.4787\n",
      "Epoch 68/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.9158 - categorical_accuracy: 0.7102\n",
      "Epoch 00068: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 53s 50ms/step - loss: 0.9158 - categorical_accuracy: 0.7102 - val_loss: 1.6006 - val_categorical_accuracy: 0.5169\n",
      "Epoch 69/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.9231 - categorical_accuracy: 0.7048\n",
      "Epoch 00069: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 49s 46ms/step - loss: 0.9231 - categorical_accuracy: 0.7048 - val_loss: 1.8701 - val_categorical_accuracy: 0.4665\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068/1068 [==============================] - ETA: 0s - loss: 0.9111 - categorical_accuracy: 0.7087\n",
      "Epoch 00070: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 51s 48ms/step - loss: 0.9111 - categorical_accuracy: 0.7087 - val_loss: 1.4522 - val_categorical_accuracy: 0.5666\n",
      "Epoch 71/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.9007 - categorical_accuracy: 0.7101\n",
      "Epoch 00071: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 56s 52ms/step - loss: 0.9006 - categorical_accuracy: 0.7101 - val_loss: 1.4215 - val_categorical_accuracy: 0.5928\n",
      "Epoch 72/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.8933 - categorical_accuracy: 0.7133\n",
      "Epoch 00072: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 51s 48ms/step - loss: 0.8933 - categorical_accuracy: 0.7133 - val_loss: 1.4935 - val_categorical_accuracy: 0.5763\n",
      "Epoch 73/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.8825 - categorical_accuracy: 0.7171\n",
      "Epoch 00073: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.8825 - categorical_accuracy: 0.7171 - val_loss: 1.3789 - val_categorical_accuracy: 0.5977\n",
      "Epoch 74/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.8830 - categorical_accuracy: 0.7197\n",
      "Epoch 00074: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.8829 - categorical_accuracy: 0.7197 - val_loss: 1.4251 - val_categorical_accuracy: 0.5893\n",
      "Epoch 75/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.8645 - categorical_accuracy: 0.7264\n",
      "Epoch 00075: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 0.8645 - categorical_accuracy: 0.7264 - val_loss: 1.4220 - val_categorical_accuracy: 0.5730\n",
      "Epoch 76/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.8657 - categorical_accuracy: 0.7255\n",
      "Epoch 00076: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 46s 43ms/step - loss: 0.8657 - categorical_accuracy: 0.7255 - val_loss: 1.7671 - val_categorical_accuracy: 0.4880\n",
      "Epoch 77/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.8601 - categorical_accuracy: 0.7243\n",
      "Epoch 00077: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 0.8603 - categorical_accuracy: 0.7242 - val_loss: 1.7748 - val_categorical_accuracy: 0.5196\n",
      "Epoch 78/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.8486 - categorical_accuracy: 0.7290\n",
      "Epoch 00078: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 0.8486 - categorical_accuracy: 0.7290 - val_loss: 1.4065 - val_categorical_accuracy: 0.6077\n",
      "Epoch 79/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.8480 - categorical_accuracy: 0.7282\n",
      "Epoch 00079: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 0.8478 - categorical_accuracy: 0.7282 - val_loss: 1.6354 - val_categorical_accuracy: 0.5231\n",
      "Epoch 80/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.8340 - categorical_accuracy: 0.7334\n",
      "Epoch 00080: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 0.8340 - categorical_accuracy: 0.7333 - val_loss: 1.3750 - val_categorical_accuracy: 0.5961\n",
      "Epoch 81/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.8356 - categorical_accuracy: 0.7315\n",
      "Epoch 00081: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 0.8354 - categorical_accuracy: 0.7316 - val_loss: 2.1487 - val_categorical_accuracy: 0.4476\n",
      "Epoch 82/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.8235 - categorical_accuracy: 0.7380\n",
      "Epoch 00082: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 44ms/step - loss: 0.8235 - categorical_accuracy: 0.7380 - val_loss: 1.3495 - val_categorical_accuracy: 0.6093\n",
      "Epoch 83/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.8185 - categorical_accuracy: 0.7335\n",
      "Epoch 00083: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.8184 - categorical_accuracy: 0.7335 - val_loss: 1.4217 - val_categorical_accuracy: 0.6119\n",
      "Epoch 84/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.8149 - categorical_accuracy: 0.7393\n",
      "Epoch 00084: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 50s 47ms/step - loss: 0.8149 - categorical_accuracy: 0.7393 - val_loss: 1.6221 - val_categorical_accuracy: 0.5633\n",
      "Epoch 85/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.8011 - categorical_accuracy: 0.7440\n",
      "Epoch 00085: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 55s 52ms/step - loss: 0.8012 - categorical_accuracy: 0.7438 - val_loss: 1.4650 - val_categorical_accuracy: 0.5902\n",
      "Epoch 86/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.7954 - categorical_accuracy: 0.7454\n",
      "Epoch 00086: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 52s 49ms/step - loss: 0.7954 - categorical_accuracy: 0.7454 - val_loss: 1.5362 - val_categorical_accuracy: 0.5707\n",
      "Epoch 87/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.7997 - categorical_accuracy: 0.7429\n",
      "Epoch 00087: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.7997 - categorical_accuracy: 0.7429 - val_loss: 1.3284 - val_categorical_accuracy: 0.6244\n",
      "Epoch 88/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.7831 - categorical_accuracy: 0.7524\n",
      "Epoch 00088: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.7831 - categorical_accuracy: 0.7524 - val_loss: 1.4145 - val_categorical_accuracy: 0.6121\n",
      "Epoch 89/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.7764 - categorical_accuracy: 0.7544\n",
      "Epoch 00089: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 51s 48ms/step - loss: 0.7764 - categorical_accuracy: 0.7543 - val_loss: 1.4983 - val_categorical_accuracy: 0.5819\n",
      "Epoch 90/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.7748 - categorical_accuracy: 0.7540\n",
      "Epoch 00090: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 53s 50ms/step - loss: 0.7752 - categorical_accuracy: 0.7539 - val_loss: 1.8476 - val_categorical_accuracy: 0.5062\n",
      "Epoch 91/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.7680 - categorical_accuracy: 0.7548\n",
      "Epoch 00091: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 50s 47ms/step - loss: 0.7680 - categorical_accuracy: 0.7548 - val_loss: 1.3441 - val_categorical_accuracy: 0.6293\n",
      "Epoch 92/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.7554 - categorical_accuracy: 0.7612\n",
      "Epoch 00092: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 50s 47ms/step - loss: 0.7554 - categorical_accuracy: 0.7612 - val_loss: 1.4626 - val_categorical_accuracy: 0.5779\n",
      "Epoch 93/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.7516 - categorical_accuracy: 0.7610\n",
      "Epoch 00093: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 50s 47ms/step - loss: 0.7516 - categorical_accuracy: 0.7609 - val_loss: 1.5916 - val_categorical_accuracy: 0.5549\n",
      "Epoch 94/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.7408 - categorical_accuracy: 0.7643\n",
      "Epoch 00094: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.7408 - categorical_accuracy: 0.7643 - val_loss: 2.0454 - val_categorical_accuracy: 0.5022\n",
      "Epoch 95/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.7314 - categorical_accuracy: 0.7678\n",
      "Epoch 00095: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.7314 - categorical_accuracy: 0.7678 - val_loss: 1.3031 - val_categorical_accuracy: 0.6427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.7223 - categorical_accuracy: 0.7712\n",
      "Epoch 00096: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 0.7223 - categorical_accuracy: 0.7712 - val_loss: 1.3963 - val_categorical_accuracy: 0.6272\n",
      "Epoch 97/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.7281 - categorical_accuracy: 0.7681\n",
      "Epoch 00097: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.7279 - categorical_accuracy: 0.7682 - val_loss: 1.9699 - val_categorical_accuracy: 0.4852\n",
      "Epoch 98/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.7209 - categorical_accuracy: 0.7701\n",
      "Epoch 00098: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 48s 45ms/step - loss: 0.7209 - categorical_accuracy: 0.7701 - val_loss: 1.6753 - val_categorical_accuracy: 0.5450\n",
      "Epoch 99/100\n",
      "1067/1068 [============================>.] - ETA: 0s - loss: 0.7124 - categorical_accuracy: 0.7710\n",
      "Epoch 00099: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 47s 44ms/step - loss: 0.7120 - categorical_accuracy: 0.7712 - val_loss: 1.5141 - val_categorical_accuracy: 0.5895\n",
      "Epoch 100/100\n",
      "1068/1068 [==============================] - ETA: 0s - loss: 0.7162 - categorical_accuracy: 0.7682\n",
      "Epoch 00100: val_loss did not improve from 1.26610\n",
      "1068/1068 [==============================] - 45s 42ms/step - loss: 0.7162 - categorical_accuracy: 0.7682 - val_loss: 1.4851 - val_categorical_accuracy: 0.6035\n",
      "        loss  categorical_accuracy  val_loss  val_categorical_accuracy     lr\n",
      "0   2.117783              0.414272  1.949494                  0.421247  0.001\n",
      "1   1.832448              0.460985  1.953737                  0.461457  0.001\n",
      "2   1.738032              0.477843  1.774559                  0.481475  0.001\n",
      "3   1.683958              0.492302  1.726331                  0.487796  0.001\n",
      "4   1.640956              0.500966  1.803413                  0.447761  0.001\n",
      "..       ...                   ...       ...                       ...    ...\n",
      "95  0.722320              0.771235  1.396296                  0.627217  0.001\n",
      "96  0.727923              0.768191  1.969922                  0.485162  0.001\n",
      "97  0.720940              0.770064  1.675325                  0.545040  0.001\n",
      "98  0.712014              0.771176  1.514052                  0.589464  0.001\n",
      "99  0.716178              0.768249  1.485149                  0.603512  0.001\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "dict_keys(['loss', 'categorical_accuracy', 'val_loss', 'val_categorical_accuracy', 'lr'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training duration: 80.55027720928192\n",
      "0.599297629499561\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gait_fcn.h5\"\n",
    "model = train_model(df_idnet, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98475263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
