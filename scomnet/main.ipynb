{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc89b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8be857af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):       \n",
    "    list_set = set(list1) \n",
    "    unique_list = (list(list_set)) \n",
    "    unique_list.sort()\n",
    "    return unique_list\n",
    "\n",
    "def create_userids( df ):\n",
    "    array = df.values\n",
    "    y = array[:, -1]\n",
    "    return unique( y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b65a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df):\n",
    "    RANDOM_STATE = 11235\n",
    "    \n",
    "    userids = create_userids(df)\n",
    "    nbclasses = len(userids)    \n",
    "    array = df.values\n",
    "    nsamples, nfeatures = array.shape\n",
    "    nfeatures = nfeatures -1 \n",
    "    X = array[:,0:nfeatures]\n",
    "    y = array[:,-1]\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y.reshape(-1,1))\n",
    "    y = enc.transform(y.reshape(-1, 1)).toarray()\n",
    "    X = X.reshape(-1, 128, 3)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)\n",
    "    \n",
    "    mini_batch_size = int(min(X_train.shape[0]/10, 32))\n",
    "        \n",
    "    X_train = np.asarray(X_train).astype(np.float32)\n",
    "    X_val = np.asarray(X_val).astype(np.float32)\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "    \n",
    "    BATCH_SIZE = mini_batch_size\n",
    "    SHUFFLE_BUFFER_SIZE = 100\n",
    "    \n",
    "    train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    val_ds = val_ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    return train_ds, val_ds, nbclasses    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b764e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    screens = ['Focus', 'Mathisis', 'Memoria', 'Reacton', 'Speedy']\n",
    "    screens_code = ['1', '2', '3', '4', '5']\n",
    "\n",
    "    base_path = \"C:/Users/SouthSystem/Federated Learning/DataBioCom/data\"\n",
    "    phone_accel_file_paths = []\n",
    "\n",
    "    for directories, subdirectories, files in os.walk(base_path):\n",
    "        for filename in files:\n",
    "            if \"accel\" in filename:\n",
    "                phone_accel_file_paths.append(f\"{base_path}/accel/{filename}\")\n",
    "\n",
    "    data = pd.concat(map(pd.read_csv, phone_accel_file_paths))\n",
    "    users = data['player_id'].unique()\n",
    "    \n",
    "    train_set, user_list = split_data(data, users)\n",
    "    train_set = np.array([np.array(x) for x in train_set]) \n",
    "    train_set_join = train_set.reshape(train_set.shape[0], 384)\n",
    "    data_join = pd.DataFrame(train_set_join)\n",
    "    data_join['user'] = user_list\n",
    "    \n",
    "    train_ds, val_ds, n = split_dataframe(data_join)\n",
    "    \n",
    "    return train_ds, val_ds, n\n",
    "    \n",
    "def split_data(data, users):\n",
    "    user_list = []\n",
    "    train = []\n",
    "    frame_size = 128\n",
    "    step = 50\n",
    "\n",
    "    for user in users:\n",
    "        data_user = data[data['player_id']==user]  \n",
    "        data_user = data_user.iloc[:,[0,1,2]]\n",
    "        for w in range(0, data_user.shape[0] - frame_size, step):\n",
    "            end = w + frame_size        \n",
    "            frame = data_user.iloc[w:end,[0, 1, 2]]        \n",
    "            train.append(frame)\n",
    "            user_list.append(user)\n",
    "\n",
    "    return train, user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6099310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    train_dataset, validation_dataset, n = load_data()\n",
    "    return train_dataset, validation_dataset, n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8a7c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized_training_loop(train_dataset, validation_dataset, nbclasses, input_shape = (128, 3), num_filters = 128):\n",
    "    input_layer = keras.layers.Input(input_shape) \n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=num_filters, kernel_size=8, padding='same')(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=2*num_filters, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(num_filters, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    output_layer = keras.layers.Dense(nbclasses, activation='softmax')(gap_layer)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    learning_rate = 0.0001\n",
    "    cb = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50,min_lr=learning_rate)\n",
    "    precision = tf.keras.metrics.Precision(name='precision')\n",
    "    recall = tf.keras.metrics.Recall(name='recall')\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics=['accuracy', precision, recall]) \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    EPOCHS = 50\n",
    "    \n",
    "    hist = model.fit(train_dataset, \n",
    "                  epochs=EPOCHS,\n",
    "                  verbose=True, \n",
    "                  validation_data=validation_dataset, \n",
    "                  callbacks=cb)\n",
    "    \n",
    "    hist_df = pd.DataFrame(hist.history) \n",
    "    \n",
    "    print(hist_df)\n",
    "    \n",
    "    validation_metrics = model.evaluate(validation_dataset, return_dict=True)\n",
    "    print(\"Evaluating validation metrics\")\n",
    "    for m in model.metrics:\n",
    "        print(f\"\\t{m.name}: {validation_metrics[m.name]:.4f}\")\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef56ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized_pipeline():\n",
    "    train_dataset, validation_dataset, n = get_datasets()\n",
    "    centralized_training_loop(train_dataset, validation_dataset, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "825b018b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128, 3)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 128, 128)          3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 128, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 128, 128)          98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                3870      \n",
      "=================================================================\n",
      "Total params: 271,646\n",
      "Trainable params: 270,622\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1881/1881 [==============================] - 139s 74ms/step - loss: 1.6197 - accuracy: 0.4921 - precision: 0.6527 - recall: 0.2914 - val_loss: 1.4447 - val_accuracy: 0.5536 - val_precision: 0.7078 - val_recall: 0.3595\n",
      "Epoch 2/50\n",
      "1881/1881 [==============================] - 167s 89ms/step - loss: 1.4649 - accuracy: 0.5305 - precision: 0.6844 - recall: 0.3530 - val_loss: 1.4551 - val_accuracy: 0.5420 - val_precision: 0.7445 - val_recall: 0.3310\n",
      "Epoch 3/50\n",
      "1881/1881 [==============================] - 177s 94ms/step - loss: 1.3817 - accuracy: 0.5558 - precision: 0.7105 - recall: 0.3887 - val_loss: 1.4308 - val_accuracy: 0.5081 - val_precision: 0.7255 - val_recall: 0.3233\n",
      "Epoch 4/50\n",
      "1881/1881 [==============================] - 173s 92ms/step - loss: 1.2978 - accuracy: 0.5806 - precision: 0.7284 - recall: 0.4293 - val_loss: 1.1956 - val_accuracy: 0.6075 - val_precision: 0.7484 - val_recall: 0.4845\n",
      "Epoch 5/50\n",
      "1881/1881 [==============================] - 169s 90ms/step - loss: 1.2273 - accuracy: 0.6010 - precision: 0.7452 - recall: 0.4591 - val_loss: 1.1053 - val_accuracy: 0.6422 - val_precision: 0.7739 - val_recall: 0.5185\n",
      "Epoch 6/50\n",
      "1881/1881 [==============================] - 155s 83ms/step - loss: 1.1801 - accuracy: 0.6153 - precision: 0.7562 - recall: 0.4856 - val_loss: 1.0528 - val_accuracy: 0.6718 - val_precision: 0.8199 - val_recall: 0.5220\n",
      "Epoch 7/50\n",
      "1881/1881 [==============================] - 149s 79ms/step - loss: 1.1328 - accuracy: 0.6328 - precision: 0.7661 - recall: 0.5088 - val_loss: 1.0376 - val_accuracy: 0.6727 - val_precision: 0.8003 - val_recall: 0.5580\n",
      "Epoch 8/50\n",
      "1881/1881 [==============================] - 154s 82ms/step - loss: 1.0966 - accuracy: 0.6423 - precision: 0.7735 - recall: 0.5242 - val_loss: 1.0070 - val_accuracy: 0.6785 - val_precision: 0.7988 - val_recall: 0.5823\n",
      "Epoch 9/50\n",
      "1881/1881 [==============================] - 155s 82ms/step - loss: 1.0649 - accuracy: 0.6547 - precision: 0.7799 - recall: 0.5415 - val_loss: 0.9543 - val_accuracy: 0.6931 - val_precision: 0.8334 - val_recall: 0.5714\n",
      "Epoch 10/50\n",
      "1881/1881 [==============================] - 155s 83ms/step - loss: 1.0446 - accuracy: 0.6581 - precision: 0.7812 - recall: 0.5484 - val_loss: 1.0723 - val_accuracy: 0.6491 - val_precision: 0.7539 - val_recall: 0.5541\n",
      "Epoch 11/50\n",
      "1881/1881 [==============================] - 154s 82ms/step - loss: 1.0185 - accuracy: 0.6677 - precision: 0.7885 - recall: 0.5612 - val_loss: 0.9463 - val_accuracy: 0.6787 - val_precision: 0.8170 - val_recall: 0.5564\n",
      "Epoch 12/50\n",
      "1881/1881 [==============================] - 153s 82ms/step - loss: 0.9967 - accuracy: 0.6742 - precision: 0.7917 - recall: 0.5741 - val_loss: 0.8989 - val_accuracy: 0.7184 - val_precision: 0.8557 - val_recall: 0.5838\n",
      "Epoch 13/50\n",
      "1881/1881 [==============================] - 156s 83ms/step - loss: 0.9771 - accuracy: 0.6786 - precision: 0.7951 - recall: 0.5805 - val_loss: 0.9623 - val_accuracy: 0.6732 - val_precision: 0.8087 - val_recall: 0.5715\n",
      "Epoch 14/50\n",
      "1881/1881 [==============================] - 185s 98ms/step - loss: 0.9599 - accuracy: 0.6848 - precision: 0.7984 - recall: 0.5875 - val_loss: 1.0075 - val_accuracy: 0.6668 - val_precision: 0.7813 - val_recall: 0.56340.9596 - accuracy: 0.6851 - pr\n",
      "Epoch 15/50\n",
      "1881/1881 [==============================] - 161s 86ms/step - loss: 0.9435 - accuracy: 0.6920 - precision: 0.8027 - recall: 0.5962 - val_loss: 0.9693 - val_accuracy: 0.6835 - val_precision: 0.7970 - val_recall: 0.6126\n",
      "Epoch 16/50\n",
      "1881/1881 [==============================] - 156s 83ms/step - loss: 0.9275 - accuracy: 0.6959 - precision: 0.8039 - recall: 0.6029 - val_loss: 1.2520 - val_accuracy: 0.6172 - val_precision: 0.7555 - val_recall: 0.5243\n",
      "Epoch 17/50\n",
      "1881/1881 [==============================] - 157s 83ms/step - loss: 0.9108 - accuracy: 0.6985 - precision: 0.8034 - recall: 0.6090 - val_loss: 0.9648 - val_accuracy: 0.6868 - val_precision: 0.7934 - val_recall: 0.6090\n",
      "Epoch 18/50\n",
      "1881/1881 [==============================] - 157s 83ms/step - loss: 0.8989 - accuracy: 0.7034 - precision: 0.8083 - recall: 0.6143 - val_loss: 1.2710 - val_accuracy: 0.5746 - val_precision: 0.7151 - val_recall: 0.4626\n",
      "Epoch 19/50\n",
      "1881/1881 [==============================] - 156s 83ms/step - loss: 0.8927 - accuracy: 0.7051 - precision: 0.8073 - recall: 0.6166 - val_loss: 1.0241 - val_accuracy: 0.6807 - val_precision: 0.7881 - val_recall: 0.6178\n",
      "Epoch 20/50\n",
      "1881/1881 [==============================] - 156s 83ms/step - loss: 0.8864 - accuracy: 0.7080 - precision: 0.8078 - recall: 0.6202 - val_loss: 1.1108 - val_accuracy: 0.6451 - val_precision: 0.7471 - val_recall: 0.5634\n",
      "Epoch 21/50\n",
      "1881/1881 [==============================] - 157s 83ms/step - loss: 0.8698 - accuracy: 0.7128 - precision: 0.8127 - recall: 0.6271 - val_loss: 1.0488 - val_accuracy: 0.6571 - val_precision: 0.7648 - val_recall: 0.5916\n",
      "Epoch 22/50\n",
      "1881/1881 [==============================] - 154s 82ms/step - loss: 0.8610 - accuracy: 0.7147 - precision: 0.8142 - recall: 0.6311 - val_loss: 1.0363 - val_accuracy: 0.6526 - val_precision: 0.7567 - val_recall: 0.5939\n",
      "Epoch 23/50\n",
      "1881/1881 [==============================] - 155s 82ms/step - loss: 0.8551 - accuracy: 0.7191 - precision: 0.8144 - recall: 0.6348 - val_loss: 1.5358 - val_accuracy: 0.5085 - val_precision: 0.6175 - val_recall: 0.4311\n",
      "Epoch 24/50\n",
      "1881/1881 [==============================] - 159s 84ms/step - loss: 0.8517 - accuracy: 0.7189 - precision: 0.8151 - recall: 0.6363 - val_loss: 0.9411 - val_accuracy: 0.6891 - val_precision: 0.7993 - val_recall: 0.6371\n",
      "Epoch 25/50\n",
      "1881/1881 [==============================] - 158s 84ms/step - loss: 0.8366 - accuracy: 0.7229 - precision: 0.8171 - recall: 0.6405 - val_loss: 0.9748 - val_accuracy: 0.6703 - val_precision: 0.7748 - val_recall: 0.5698\n",
      "Epoch 26/50\n",
      "1881/1881 [==============================] - 160s 85ms/step - loss: 0.8339 - accuracy: 0.7242 - precision: 0.8180 - recall: 0.6443 - val_loss: 0.7822 - val_accuracy: 0.7399 - val_precision: 0.8283 - val_recall: 0.6596\n",
      "Epoch 27/50\n",
      "1881/1881 [==============================] - 154s 82ms/step - loss: 0.8236 - accuracy: 0.7279 - precision: 0.8222 - recall: 0.6478 - val_loss: 0.9794 - val_accuracy: 0.6805 - val_precision: 0.7854 - val_recall: 0.6124\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881/1881 [==============================] - 153s 81ms/step - loss: 0.8179 - accuracy: 0.7299 - precision: 0.8228 - recall: 0.6507 - val_loss: 1.0461 - val_accuracy: 0.6879 - val_precision: 0.7868 - val_recall: 0.6146\n",
      "Epoch 29/50\n",
      "1881/1881 [==============================] - 158s 84ms/step - loss: 0.8126 - accuracy: 0.7305 - precision: 0.8230 - recall: 0.6533 - val_loss: 0.8392 - val_accuracy: 0.7372 - val_precision: 0.8268 - val_recall: 0.6598\n",
      "Epoch 30/50\n",
      "1881/1881 [==============================] - 154s 82ms/step - loss: 0.8069 - accuracy: 0.7320 - precision: 0.8227 - recall: 0.6552 - val_loss: 1.0862 - val_accuracy: 0.6490 - val_precision: 0.7348 - val_recall: 0.5796\n",
      "Epoch 31/50\n",
      "1881/1881 [==============================] - 155s 82ms/step - loss: 0.8038 - accuracy: 0.7330 - precision: 0.8233 - recall: 0.6567 - val_loss: 1.3228 - val_accuracy: 0.5767 - val_precision: 0.6793 - val_recall: 0.4896\n",
      "Epoch 32/50\n",
      "1881/1881 [==============================] - 162s 86ms/step - loss: 0.7983 - accuracy: 0.7375 - precision: 0.8260 - recall: 0.6600 - val_loss: 1.3089 - val_accuracy: 0.5986 - val_precision: 0.7333 - val_recall: 0.5268\n",
      "Epoch 33/50\n",
      "1881/1881 [==============================] - 166s 88ms/step - loss: 0.7916 - accuracy: 0.7373 - precision: 0.8256 - recall: 0.6620 - val_loss: 1.1662 - val_accuracy: 0.6089 - val_precision: 0.6928 - val_recall: 0.5556\n",
      "Epoch 34/50\n",
      "1881/1881 [==============================] - 167s 89ms/step - loss: 0.7853 - accuracy: 0.7372 - precision: 0.8273 - recall: 0.6634 - val_loss: 0.9464 - val_accuracy: 0.6667 - val_precision: 0.7652 - val_recall: 0.6083\n",
      "Epoch 35/50\n",
      "1881/1881 [==============================] - 171s 91ms/step - loss: 0.7808 - accuracy: 0.7413 - precision: 0.8291 - recall: 0.6664 - val_loss: 1.1535 - val_accuracy: 0.6691 - val_precision: 0.7600 - val_recall: 0.6163\n",
      "Epoch 36/50\n",
      "1881/1881 [==============================] - 168s 89ms/step - loss: 0.7769 - accuracy: 0.7408 - precision: 0.8275 - recall: 0.6671 - val_loss: 1.0308 - val_accuracy: 0.6624 - val_precision: 0.7587 - val_recall: 0.6010\n",
      "Epoch 37/50\n",
      "1881/1881 [==============================] - 169s 90ms/step - loss: 0.7738 - accuracy: 0.7430 - precision: 0.8283 - recall: 0.6685 - val_loss: 0.9414 - val_accuracy: 0.6888 - val_precision: 0.8009 - val_recall: 0.6160\n",
      "Epoch 38/50\n",
      "1881/1881 [==============================] - 168s 89ms/step - loss: 0.7676 - accuracy: 0.7453 - precision: 0.8304 - recall: 0.6719 - val_loss: 0.9748 - val_accuracy: 0.6629 - val_precision: 0.7624 - val_recall: 0.5821\n",
      "Epoch 39/50\n",
      "1881/1881 [==============================] - 169s 90ms/step - loss: 0.7501 - accuracy: 0.7495 - precision: 0.8341 - recall: 0.6773 - val_loss: 0.9363 - val_accuracy: 0.7004 - val_precision: 0.7910 - val_recall: 0.6537\n",
      "Epoch 40/50\n",
      "1881/1881 [==============================] - 169s 90ms/step - loss: 0.7644 - accuracy: 0.7464 - precision: 0.8307 - recall: 0.6756 - val_loss: 1.1360 - val_accuracy: 0.6292 - val_precision: 0.7142 - val_recall: 0.5718\n",
      "Epoch 41/50\n",
      "1881/1881 [==============================] - 170s 90ms/step - loss: 0.7520 - accuracy: 0.7496 - precision: 0.8340 - recall: 0.6792 - val_loss: 0.8604 - val_accuracy: 0.7197 - val_precision: 0.8121 - val_recall: 0.6724\n",
      "Epoch 42/50\n",
      "1881/1881 [==============================] - 170s 90ms/step - loss: 0.7522 - accuracy: 0.7494 - precision: 0.8334 - recall: 0.6787 - val_loss: 1.8504 - val_accuracy: 0.4961 - val_precision: 0.6263 - val_recall: 0.4177\n",
      "Epoch 43/50\n",
      "1881/1881 [==============================] - 173s 92ms/step - loss: 0.7496 - accuracy: 0.7495 - precision: 0.8349 - recall: 0.6801 - val_loss: 0.8443 - val_accuracy: 0.6997 - val_precision: 0.7963 - val_recall: 0.6258\n",
      "Epoch 44/50\n",
      "1881/1881 [==============================] - 174s 92ms/step - loss: 0.7376 - accuracy: 0.7522 - precision: 0.8353 - recall: 0.6842 - val_loss: 1.2491 - val_accuracy: 0.5793 - val_precision: 0.6769 - val_recall: 0.4944\n",
      "Epoch 45/50\n",
      "1881/1881 [==============================] - 173s 92ms/step - loss: 0.7427 - accuracy: 0.7534 - precision: 0.8343 - recall: 0.6852 - val_loss: 0.9194 - val_accuracy: 0.7099 - val_precision: 0.8067 - val_recall: 0.6557\n",
      "Epoch 46/50\n",
      "1881/1881 [==============================] - 167s 89ms/step - loss: 0.7376 - accuracy: 0.7553 - precision: 0.8364 - recall: 0.6848 - val_loss: 1.7251 - val_accuracy: 0.5128 - val_precision: 0.6370 - val_recall: 0.4336\n",
      "Epoch 47/50\n",
      "1881/1881 [==============================] - 172s 91ms/step - loss: 0.7339 - accuracy: 0.7546 - precision: 0.8364 - recall: 0.6865 - val_loss: 1.0667 - val_accuracy: 0.6790 - val_precision: 0.7673 - val_recall: 0.6293\n",
      "Epoch 48/50\n",
      "1881/1881 [==============================] - 169s 90ms/step - loss: 0.7249 - accuracy: 0.7576 - precision: 0.8400 - recall: 0.6896 - val_loss: 1.1216 - val_accuracy: 0.6185 - val_precision: 0.7025 - val_recall: 0.5681\n",
      "Epoch 49/50\n",
      "1881/1881 [==============================] - 167s 89ms/step - loss: 0.7236 - accuracy: 0.7589 - precision: 0.8392 - recall: 0.6907 - val_loss: 0.8151 - val_accuracy: 0.7225 - val_precision: 0.8170 - val_recall: 0.6607\n",
      "Epoch 50/50\n",
      "1881/1881 [==============================] - 166s 88ms/step - loss: 0.7134 - accuracy: 0.7628 - precision: 0.8417 - recall: 0.6958 - val_loss: 2.2470 - val_accuracy: 0.4515 - val_precision: 0.5631 - val_recall: 0.3930\n",
      "        loss  accuracy  precision    recall  val_loss  val_accuracy  \\\n",
      "0   1.619665  0.492147   0.652681  0.291382  1.444735      0.553578   \n",
      "1   1.464936  0.530508   0.684380  0.352979  1.455088      0.541960   \n",
      "2   1.381749  0.555805   0.710502  0.388748  1.430807      0.508103   \n",
      "3   1.297779  0.580587   0.728432  0.429286  1.195640      0.607479   \n",
      "4   1.227326  0.601031   0.745231  0.459104  1.105272      0.642184   \n",
      "5   1.180064  0.615341   0.756245  0.485598  1.052810      0.671803   \n",
      "6   1.132838  0.632760   0.766112  0.508768  1.037624      0.672650   \n",
      "7   1.096628  0.642300   0.773540  0.524192  1.006999      0.678484   \n",
      "8   1.064908  0.654666   0.779930  0.541511  0.954270      0.693144   \n",
      "9   1.044627  0.658140   0.781211  0.548425  1.072289      0.649115   \n",
      "10  1.018458  0.667664   0.788506  0.561240  0.946295      0.678684   \n",
      "11  0.996737  0.674229   0.791712  0.574088  0.898857      0.718424   \n",
      "12  0.977060  0.678600   0.795065  0.580537  0.962281      0.673199   \n",
      "13  0.959911  0.684817   0.798410  0.587451  1.007474      0.666816   \n",
      "14  0.943462  0.692047   0.802667  0.596161  0.969261      0.683520   \n",
      "15  0.927494  0.695853   0.803923  0.602892  1.251979      0.617153   \n",
      "16  0.910840  0.698496   0.803398  0.609042  0.964786      0.686811   \n",
      "17  0.898862  0.703449   0.808284  0.614277  1.270991      0.574570   \n",
      "18  0.892718  0.705078   0.807303  0.616604  1.024084      0.680728   \n",
      "19  0.886447  0.708036   0.807829  0.620161  1.110813      0.645076   \n",
      "20  0.869832  0.712840   0.812656  0.627109  1.048818      0.657093   \n",
      "21  0.860979  0.714651   0.814199  0.631114  1.036329      0.652605   \n",
      "22  0.855058  0.719056   0.814371  0.634821  1.535834      0.508502   \n",
      "23  0.851692  0.718906   0.815100  0.636284  0.941133      0.689105   \n",
      "24  0.836630  0.722895   0.817054  0.640539  0.974797      0.670257   \n",
      "25  0.833942  0.724159   0.817965  0.644311  0.782227      0.739915   \n",
      "26  0.823568  0.727915   0.822177  0.647752  0.979441      0.680479   \n",
      "27  0.817879  0.729876   0.822811  0.650727  1.046095      0.687908   \n",
      "28  0.812595  0.730475   0.822952  0.653287  0.839179      0.737223   \n",
      "29  0.806945  0.731970   0.822666  0.655248  1.086151      0.649015   \n",
      "30  0.803818  0.732951   0.823266  0.656711  1.322754      0.576664   \n",
      "31  0.798310  0.737538   0.825994  0.659985  1.308908      0.598604   \n",
      "32  0.791615  0.737289   0.825598  0.662029  1.166223      0.608876   \n",
      "33  0.785342  0.737206   0.827325  0.663359  0.946446      0.666717   \n",
      "34  0.780826  0.741311   0.829077  0.666417  1.153503      0.669110   \n",
      "35  0.776866  0.740763   0.827516  0.667115  1.030818      0.662428   \n",
      "36  0.773792  0.742974   0.828288  0.668495  0.941401      0.688756   \n",
      "37  0.767583  0.745267   0.830396  0.671852  0.974831      0.662927   \n",
      "38  0.750067  0.749522   0.834138  0.677321  0.936326      0.700424   \n",
      "39  0.764352  0.746414   0.830745  0.675642  1.135986      0.629170   \n",
      "40  0.751997  0.749572   0.833969  0.679249  0.860388      0.719721   \n",
      "41  0.752207  0.749422   0.833374  0.678667  1.850388      0.496136   \n",
      "42  0.749636  0.749522   0.834901  0.680146  0.844287      0.699676   \n",
      "43  0.737565  0.752248   0.835346  0.684202  1.249080      0.579257   \n",
      "44  0.742660  0.753428   0.834278  0.685199  0.919384      0.709948   \n",
      "45  0.737627  0.755256   0.836406  0.684750  1.725148      0.512790   \n",
      "46  0.733853  0.754641   0.836415  0.686495  1.066692      0.679033   \n",
      "47  0.724864  0.757633   0.839981  0.689604  1.121618      0.618549   \n",
      "48  0.723611  0.758863   0.839203  0.690667  0.815148      0.722463   \n",
      "49  0.713436  0.762786   0.841701  0.695787  2.247047      0.451458   \n",
      "\n",
      "    val_precision  val_recall     lr  \n",
      "0        0.707834    0.359511  0.001  \n",
      "1        0.744504    0.330990  0.001  \n",
      "2        0.725492    0.323261  0.001  \n",
      "3        0.748421    0.484468  0.001  \n",
      "4        0.773891    0.518474  0.001  \n",
      "5        0.819926    0.521965  0.001  \n",
      "6        0.800257    0.557966  0.001  \n",
      "7        0.798837    0.582349  0.001  \n",
      "8        0.833442    0.571379  0.001  \n",
      "9        0.753867    0.554076  0.001  \n",
      "10       0.817030    0.556420  0.001  \n",
      "11       0.855733    0.583844  0.001  \n",
      "12       0.808664    0.571528  0.001  \n",
      "13       0.781274    0.563351  0.001  \n",
      "14       0.797003    0.612565  0.001  \n",
      "15       0.755497    0.524308  0.001  \n",
      "16       0.793361    0.608975  0.001  \n",
      "17       0.715101    0.462578  0.001  \n",
      "18       0.788054    0.617751  0.001  \n",
      "19       0.747140    0.563401  0.001  \n",
      "20       0.764778    0.591573  0.001  \n",
      "21       0.756719    0.593867  0.001  \n",
      "22       0.617500    0.431065  0.001  \n",
      "23       0.799274    0.637148  0.001  \n",
      "24       0.774780    0.569833  0.001  \n",
      "25       0.828303    0.659586  0.001  \n",
      "26       0.785381    0.612366  0.001  \n",
      "27       0.786799    0.614610  0.001  \n",
      "28       0.826793    0.659786  0.001  \n",
      "29       0.734796    0.579556  0.001  \n",
      "30       0.679328    0.489604  0.001  \n",
      "31       0.733255    0.526751  0.001  \n",
      "32       0.692800    0.555622  0.001  \n",
      "33       0.765211    0.608277  0.001  \n",
      "34       0.759993    0.616255  0.001  \n",
      "35       0.758734    0.601047  0.001  \n",
      "36       0.800895    0.615956  0.001  \n",
      "37       0.762409    0.582099  0.001  \n",
      "38       0.791046    0.653702  0.001  \n",
      "39       0.714188    0.571778  0.001  \n",
      "40       0.812056    0.672401  0.001  \n",
      "41       0.626271    0.417701  0.001  \n",
      "42       0.796282    0.625829  0.001  \n",
      "43       0.676860    0.494440  0.001  \n",
      "44       0.806711    0.655747  0.001  \n",
      "45       0.636996    0.433558  0.001  \n",
      "46       0.767281    0.629319  0.001  \n",
      "47       0.702509    0.568138  0.001  \n",
      "48       0.817005    0.660733  0.001  \n",
      "49       0.563130    0.392969  0.001  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627/627 [==============================] - 13s 21ms/step - loss: 2.2470 - accuracy: 0.4515 - precision: 0.5631 - recall: 0.3930\n",
      "Evaluating validation metrics\n",
      "\tloss: 2.2470\n",
      "\taccuracy: 0.4515\n",
      "\tprecision: 0.5631\n",
      "\trecall: 0.3930\n"
     ]
    }
   ],
   "source": [
    "centralized_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426edcba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
