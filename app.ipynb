{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355ea07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45082a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./data\"\n",
    "phone_accel_file_paths = []\n",
    "phone_gyro_file_paths = []\n",
    "\n",
    "for directories, subdirectories, files in os.walk(base_path):\n",
    "    for filename in files:\n",
    "        if \"accel\" in filename:\n",
    "            phone_accel_file_paths.append(f\"{base_path}/accel/{filename}\")\n",
    "        elif \"gyro\" in filename:\n",
    "            phone_gyro_file_paths.append(f\"{base_path}/gyro/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03101daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_dict = {\"1\":\"Focus\",\n",
    "                \"2\":\"Mathisis\",\n",
    "                \"3\":\"Memoria\",\n",
    "                \"4\":\"Reacton\",\n",
    "                \"5\":\"Speedy\"\n",
    "              }\n",
    "screens = [\"Focus\", \"Mathisis\", \"Memoria\", \"Reacton\", \"Speedy\"]\n",
    "screens_code = [\"1\", \"2\", \"3\", \"4\", \"5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "f3ce4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(dataframe):\n",
    "    cleaned_df = dataframe.drop([\"player_id\", \"timestamp\"], axis = 1).copy()\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "def scale_data(data):\n",
    "    \"\"\" Normalizes the data using StandardScaler() function \"\"\"\n",
    "    \n",
    "    data.columns = ['acc_X', 'acc_Y', 'acc_Z', 'gyr_X', 'gyr_Y', 'gyr_Z', 'Screen']\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    data['s'] = le.fit_transform(data['Screen'])\n",
    "    \n",
    "    X = data[['acc_X', 'acc_Y', 'acc_Z', 'gyr_X', 'gyr_Y', 'gyr_Z']]\n",
    "    y = data['Screen']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    scaled_df = pd.DataFrame(data = X, columns = ['acc_X', 'acc_Y', 'acc_Z', 'gyr_X', 'gyr_Y', 'gyr_Z'])\n",
    "    scaled_df['Screen'] = y.values\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "\n",
    "\n",
    "def activity_dictionary(dataframe):    \n",
    "    \"\"\" Decodes the activity labels and stores them in the dictionary \"\"\"\n",
    "\n",
    "    activity_labels = dataframe[\"screen\"]\n",
    "    le = LabelEncoder()\n",
    "    activity_indices = le.fit_transform(activity_labels)\n",
    "    mapped_labels = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "    \n",
    "    return mapped_labels\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(dataframe):    \n",
    "    \"\"\" Preprocesses the data using balance(), clean(), and scale() functions \"\"\"\n",
    "\n",
    "    cleaned_df = clean_data(dataframe)\n",
    "    \n",
    "    return scale_data(cleaned_df)\n",
    "\n",
    "\n",
    "def plot_learningCurve(history, epochs):\n",
    "    \"\"\" Plots training & validation accuracy values \"\"\"\n",
    "\n",
    "    epoch_range = range(1, epochs+1)\n",
    "    plt.plot(epoch_range, history.history['accuracy'])\n",
    "    plt.plot(epoch_range, history.history[\"val_accuracy\"])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    \"\"\" Plots training & validation loss values \"\"\"\n",
    "    \n",
    "    plt.plot(epoch_range, history.history['loss'])\n",
    "    plt.plot(epoch_range, history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "8ef2729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Divides data into 2D frames \"\"\"\n",
    "\n",
    "frequency = 10 # Based on Hertz\n",
    "time_period = 10 # Based on Second\n",
    "frame_size = frequency * time_period\n",
    "step_size = frame_size # In order not to have an overlap\n",
    "\n",
    "def get_frames(df, frame_size, step_size):\n",
    "    n_features = 6\n",
    "    frames = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - frame_size, step_size):\n",
    "        acc_x = df['acc_X'].values[i: i + frame_size]\n",
    "        acc_y = df['acc_Y'].values[i: i + frame_size]\n",
    "        acc_z = df['acc_Z'].values[i: i + frame_size]\n",
    "        gyr_x = df['gyr_X'].values[i: i + frame_size]\n",
    "        gyr_y = df['gyr_Y'].values[i: i + frame_size]\n",
    "        gyr_z = df['gyr_Z'].values[i: i + frame_size]\n",
    "        \n",
    "        label = stats.mode(df['Screen'][i: i + frame_size])[0][0]\n",
    "        frames.append([acc_x, acc_y, acc_z, gyr_x, gyr_y, gyr_z])\n",
    "        labels.append(label)\n",
    "\n",
    "    frames = np.asarray(frames).reshape(-1, frame_size, n_features)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    print(frames.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    return frames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82980067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  \n",
    "    s0 = nn.Conv2d(1, 32, (1, 3))\n",
    "    s1 = nn.ReLU()\n",
    "    s2 = nn.MaxPool2d((1, 2), 2)\n",
    "    s3 = nn.Conv2d(32, 64, (1, 3))\n",
    "    s4 = nn.ReLU()\n",
    "    s5 = nn.MaxPool2d((1, 2), 2)         \n",
    "    s6 = nn.Flatten()\n",
    "    s7 = nn.Linear(64 * 23 , 64)\n",
    "    s8 = nn.ReLU()\n",
    "    s9 = torch.nn.Dropout(p=0.5)\n",
    "    s10 = nn.Linear(64, 16)\n",
    "    s11 = nn.Softmax()\n",
    "\n",
    "    model = nn.Sequential(s0,s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb87a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_accel_accuracy = {}\n",
    "phone_accel_precision = {}\n",
    "phone_accel_recall = {}\n",
    "phone_accel_f1 = {}\n",
    "\n",
    "phone_accel_matrix = {}\n",
    "phone_accel_activity_accuracy = {}\n",
    "phone_accel_classification_reports={}\n",
    "\n",
    "data = pd.concat(map(pd.read_csv, phone_accel_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_acc_gyr = pd.read_csv('data/data_all.csv')\n",
    "data_acc_gyr['player_id'].value_counts()\n",
    "data = data_acc_gyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "17cb965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_x, train_y):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output_train = model(train_x)\n",
    "    loss_train = criterion(output_train, train_y)\n",
    "    train_losses.append(loss_train)\n",
    "\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "\n",
    "    print('Epoch : ',epoch+1, '\\t', 'loss :', tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa08f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectIDs = data[\"player_id\"].unique()\n",
    "subjectIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d0f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(screens):\n",
    "    data.loc[data.screen.str.contains(screens[idx]), 'screen'] = screens_code[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "9fc75e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = data[['screen']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "6b47456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, label_train, label_test = train_test_split(\n",
    "data, all_labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "43f94059",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_data = preprocess_data(train_data)\n",
    "processed_test_data = preprocess_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dcc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_frames(processed_train_data, frame_size, step_size)\n",
    "X_test, y_test = get_frames(processed_test_data, frame_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ba572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, 6, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 6, X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "78966d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = X_train.astype(np.float32)\n",
    "train_x = torch.from_numpy(train_x)\n",
    "    \n",
    "train_y = y_train.astype(np.float64)\n",
    "train_y = torch.from_numpy(train_y)\n",
    "train_y = train_y.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "1c871c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = X_test.astype(np.float32)\n",
    "test_x = torch.from_numpy(test_x)\n",
    "    \n",
    "test_y = y_test.astype(np.float64)\n",
    "test_y = torch.from_numpy(test_y)\n",
    "test_y = test_y.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2738b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch, train_x, train_y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(train_x)\n",
    "    \n",
    "softmax = torch.exp(output).cpu()\n",
    "prob = list(softmax.detach().numpy())\n",
    "predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# accuracy on training set\n",
    "accuracy_score(train_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_los = [fl.item() for fl in train_losses ]\n",
    "plt.plot(range(n_epochs), fi_los)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "961f1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_groups = data_iid(train_x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "905be550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iid(dataset, num_users):\n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
    "                                             replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d391a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "85a5a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalUpdate(object):\n",
    "    def __init__(self, dataset, label, idxs):        \n",
    "        self.trainloader, self.validloader, self.testloader = self.train_val_test(\n",
    "            dataset, label, list(idxs))\n",
    "        self.device = 'cpu'\n",
    "        # Default criterion set to NLL loss function\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "    def train_val_test(self, dataset, label, idxs):\n",
    "        \"\"\"\n",
    "        Returns train, validation and test dataloaders for a given dataset\n",
    "        and user indexes.\n",
    "        \"\"\"\n",
    "        idxs = np.asarray(idxs)  \n",
    "        \n",
    "        # split indexes for train, validation, and test (80, 10, 10)\n",
    "        idxs_train = idxs[:int(0.8*len(idxs))]\n",
    "        idxs_val = idxs[int(0.8*len(idxs)):int(0.9*len(idxs))]\n",
    "        idxs_test = idxs[int(0.9*len(idxs)):]    \n",
    "       \n",
    "        trainloader = DataLoader(DatasetSplit(dataset, label, idxs_train),\n",
    "                                 batch_size=64, shuffle=True)\n",
    "        validloader = DataLoader(DatasetSplit(dataset, label, idxs_val),\n",
    "                                 batch_size=int(len(idxs_val)/10), shuffle=False)\n",
    "        testloader = DataLoader(DatasetSplit(dataset, label, idxs_test),\n",
    "                                batch_size=int(len(idxs_test)/10), shuffle=False)\n",
    "        return trainloader, validloader, testloader\n",
    "    \n",
    "    def update_weights(self, model, global_round):\n",
    "        # Set mode to train model\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        for iter in range(10):\n",
    "            batch_loss = []\n",
    "            for batch_idx, (images, labels) in enumerate(self.trainloader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                model.zero_grad()\n",
    "                log_probs = model(images)\n",
    "                loss = self.criterion(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (batch_idx % 300 == 0):\n",
    "                    print('| Global Round : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        global_round, iter, batch_idx * len(images),\n",
    "                        len(self.trainloader.dataset),\n",
    "                        100. * batch_idx / len(self.trainloader), loss.item()))                \n",
    "                batch_loss.append(loss.item())\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "\n",
    "        return model.state_dict(), sum(epoch_loss) / len(epoch_loss)\n",
    "    \n",
    "    def inference(self, model):\n",
    "        \"\"\" Returns the inference accuracy and loss.\n",
    "        \"\"\"\n",
    "\n",
    "        model.eval()\n",
    "        loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(self.testloader):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "            # Inference\n",
    "            outputs = model(images)\n",
    "            batch_loss = self.criterion(outputs, labels)\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "            # Prediction\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "            total += len(labels)\n",
    "\n",
    "        accuracy = correct/total\n",
    "        return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbef4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, label, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.label = label\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        label = self.label[self.idxs[item]]\n",
    "        image = self.dataset[self.idxs[item]]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "e6ac5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(1,2))\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(1,2))\n",
    "        self.conv2_drop = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(128 * 588, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 1)\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "92be7c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_x = train_x\n",
    "train_dataset_y = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63608e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "global_model = CNNModel()\n",
    "\n",
    "global_model.to(device)\n",
    "global_model.train()\n",
    "print(global_model)\n",
    "\n",
    "# copy weights\n",
    "global_weights = global_model.state_dict()\n",
    "\n",
    "# Training\n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 2\n",
    "val_loss_pre, counter = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(10)):\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "\n",
    "    global_model.train()\n",
    "    m = max(int(0.1 * 10), 1)\n",
    "    idxs_users = np.random.choice(range(10), m, replace=False)\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local_model = LocalUpdate(dataset=train_dataset_x, label = train_dataset_y,\n",
    "                                  idxs=user_groups[idx])\n",
    "        w, loss = local_model.update_weights(\n",
    "            model=copy.deepcopy(global_model), global_round=epoch)\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "    # update global weights\n",
    "    global_weights = average_weights(local_weights)\n",
    "\n",
    "    # update global weights\n",
    "    global_model.load_state_dict(global_weights)\n",
    "\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # Calculate avg training accuracy over all users at every epoch\n",
    "    list_acc, list_loss = [], []\n",
    "    global_model.eval()\n",
    "    for c in range(10):\n",
    "        local_model = LocalUpdate(dataset=train_dataset_x, label = train_dataset_y,\n",
    "                                  idxs=user_groups[idx])\n",
    "        acc, loss = local_model.inference(model=global_model)\n",
    "        list_acc.append(acc)\n",
    "        list_loss.append(loss)\n",
    "    train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "\n",
    "    # print global training loss after every 'i' rounds\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "deab52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(model, test_dataset):\n",
    "    \"\"\" Returns the test accuracy and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "    device = 'cpu'\n",
    "    criterion = nn.NLLLoss().to(device)\n",
    "    testloader = DataLoader(test_dataset, batch_size=115,\n",
    "                            shuffle=False)\n",
    "\n",
    "    for batch_idx, images in enumerate(testloader):        \n",
    "        images = images.to(device)\n",
    "        \n",
    "        #print(images.shape)\n",
    "        \n",
    "        labels = test_y_shape[batch_idx] \n",
    "        print(labels.shape)\n",
    "        labels = labels.to(device)\n",
    "       \n",
    "        # Inference\n",
    "        outputs = model(images)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Prediction\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "\n",
    "    accuracy = correct/total\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference after completion of training\n",
    "test_acc, test_loss = test_inference(global_model, test_x)\n",
    "\n",
    "print(f' \\n Results after {10} global rounds of training:')\n",
    "print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_shape = test_y.reshape(-1, 115)\n",
    "test_y_shape[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "dc1d2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING (optional)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Loss curve\n",
    "plt.figure()\n",
    "plt.title('Training Loss vs Communication rounds')\n",
    "plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "plt.ylabel('Training loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig(\"./data/fed_loss.png\")\n",
    "\n",
    "# # Plot Average Accuracy vs Communication rounds\n",
    "plt.figure()\n",
    "plt.title('Average Accuracy vs Communication rounds')\n",
    "plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig(\"./data/fed_acc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13756d37",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
